{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T01:52:29.700548Z",
     "start_time": "2025-10-19T01:52:26.756854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                  AIVI DATA INTEGRATION - BLOCO 1 v4.1                        ║\n",
    "║              CONFIGURAÇÃO COMPLETA DO AMBIENTE + UX/UI + LOGS                ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║  Versão: 4.1 - Consolidado, Modular, Inteligente                            ║\n",
    "║  Data: 2025-01-15                                                            ║\n",
    "║  Prompt Base: AIVI v4.1 (com UX/UI, Schema Logging, Exploração)             ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║  CONTEÚDO DESTE BLOCO:                                                       ║\n",
    "║    1. Imports e Constantes AIVI 2025                                         ║\n",
    "║    2. Sistema UX/UI com Timer (PARTE 0.10)                                   ║\n",
    "║    3. Log de Campos + Exploração (PARTE 12.3 + 14)                           ║\n",
    "║    4. Regex Patterns para Detecção de Conteúdo                               ║\n",
    "║    5. Validações Não-Destrutivas                                             ║\n",
    "║    6. FileManager Expandido                                                  ║\n",
    "║    7. Inicialização e Teste                                                  ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║  PROTOCOLO BLOCO-A-BLOCO:                                                    ║\n",
    "║    ✅ Execute este bloco                                                     ║\n",
    "║    ✅ Valide todos os outputs                                                ║\n",
    "║    ✅ Teste a GUI (janela de seleção de pasta)                               ║\n",
    "║    ✅ Confirme: \"BLOCO 1 OK\" ou reporte problemas                            ║\n",
    "║    ⏭️  Só então prosseguiremos para BLOCO 2                                  ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 1: IMPORTS E CONFIGURAÇÕES BÁSICAS\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"╔\" + \"═\" * 78 + \"╗\")\n",
    "print(\"║\" + \" 🚀 BLOCO 1: CONFIGURAÇÃO COMPLETA DO AMBIENTE\".center(78) + \"║\")\n",
    "print(\"╚\" + \"═\" * 78 + \"╝\")\n",
    "print()\n",
    "\n",
    "# Imports padrão\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Imports para análise de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Imports para GUI\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "# Configurações\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.4f}')\n",
    "\n",
    "print(\"📦 SEÇÃO 1.1: Imports\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   ✅ Bibliotecas padrão: sys, os, json, logging, datetime\")\n",
    "print(\"   ✅ Análise de dados: pandas, numpy\")\n",
    "print(\"   ✅ GUI: tkinter\")\n",
    "print(\"   ✅ Configurações aplicadas\")\n",
    "print()\n",
    "\n",
    "# Verificar versões\n",
    "modulos_carregados = {\n",
    "    'Python': sys.version.split()[0],\n",
    "    'Pandas': pd.__version__,\n",
    "    'NumPy': np.__version__\n",
    "}\n",
    "\n",
    "print(\"📋 Versões:\")\n",
    "for modulo, versao in modulos_carregados.items():\n",
    "    print(f\"   • {modulo}: {versao}\")\n",
    "print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 2: CONSTANTES AIVI 2025\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"📊 SEÇÃO 1.2: Constantes AIVI 2025\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Constantes principais\n",
    "CONSTANTES_AIVI = {\n",
    "    'ANO_VIGENCIA': 2025,\n",
    "    'VERSAO_SISTEMA': '4.1',\n",
    "    'DATA_ATUALIZACAO': '2025-01-15',\n",
    "\n",
    "    # Parâmetros estatísticos\n",
    "    'PERCENTIL_INFERIOR': 10,\n",
    "    'PERCENTIL_SUPERIOR': 90,\n",
    "    'DESVIOS_PADRAO_BATENTE': 2.5,\n",
    "\n",
    "    # Pesos AIVI\n",
    "    'PESO_CONFORMIDADE': 0.50,\n",
    "    'PESO_SEVERIDADE': 0.30,\n",
    "    'PESO_FREQUENCIA': 0.20,\n",
    "\n",
    "    # Limites de alerta\n",
    "    'LIMITE_CRITICO_PCT': 30.0,\n",
    "    'LIMITE_ALERTA_PCT': 10.0,\n",
    "\n",
    "    # Configurações de análise\n",
    "    'MESES_MINIMO_HISTORICO': 6,\n",
    "    'MESES_IDEAL_HISTORICO': 12,\n",
    "    'CONFIANCA_MINIMA_MAPEAMENTO': 0.80\n",
    "}\n",
    "\n",
    "print(f\"   ✅ Ano de vigência: {CONSTANTES_AIVI['ANO_VIGENCIA']}\")\n",
    "print(f\"   ✅ Versão do sistema: {CONSTANTES_AIVI['VERSAO_SISTEMA']}\")\n",
    "print(f\"   ✅ {len(CONSTANTES_AIVI)} constantes definidas\")\n",
    "print()\n",
    "\n",
    "# Schema AIVI - Campos obrigatórios\n",
    "SCHEMA_AIVI_CAMPOS_OBRIGATORIOS = [\n",
    "    'Período',\n",
    "    'Centro',\n",
    "    'Sigla',\n",
    "    'Cód Grupo de produto',\n",
    "    'Expedição c/ Veículo',\n",
    "    'Variação Interna',\n",
    "    '% VI',\n",
    "    'LI Atual (%)',\n",
    "    'LS Atual (%)'\n",
    "]\n",
    "\n",
    "print(f\"   ✅ Schema AIVI: {len(SCHEMA_AIVI_CAMPOS_OBRIGATORIOS)} campos obrigatórios\")\n",
    "print()\n",
    "\n",
    "# Dicionário de sinônimos expandido\n",
    "DICIONARIO_SINONIMOS = {\n",
    "    'Centro': [\n",
    "        'Centro', 'Código de Centro', 'Cod Centro', 'CódigoCentro',\n",
    "        'Centro Operacional', 'ID Centro', 'Centro_ID', 'CENTRO',\n",
    "        'Cod_Centro', 'Centro_Operacional', 'Plant', 'Planta'\n",
    "    ],\n",
    "\n",
    "    'Sigla': [\n",
    "        'Sigla', 'Sigla de Centro', 'Sigla Centro', 'Sigla Base',\n",
    "        'Base', 'Sigla_Centro', 'SIGLA', 'SiglaBase', 'Sigla_Base',\n",
    "        'Sigla do Centro', 'Nome Base', 'Base Operacional'\n",
    "    ],\n",
    "\n",
    "    'Cód Grupo de produto': [\n",
    "        'Cód Grupo de produto', 'Código Produto', 'Cod Produto',\n",
    "        'Grupo Produto', 'CodGrupoProduto', 'Cód_Grupo_produto',\n",
    "        'PRODUTO', 'Produto_Cod', 'CodGrupo', 'Material Group',\n",
    "        'Grupo de Material', 'Product Group'\n",
    "    ],\n",
    "\n",
    "    'Expedição c/ Veículo': [\n",
    "        'Expedição c/ Veículo', 'Expedição', 'Expedicao',\n",
    "        'Volume Expedido', 'Exped_Veiculo', 'EXPEDICAO', 'Exped',\n",
    "        'Expedicao_Veiculo', 'Volume_Expedido', 'Dispatch Volume',\n",
    "        'Volume Carregado', 'Carregamento'\n",
    "    ],\n",
    "\n",
    "    'Variação Interna': [\n",
    "        'Variação Interna', 'Variacao Interna', 'VI', 'Var Interna',\n",
    "        'VarInt', 'VARIACAO', 'Var_Interna', 'VariacaoInterna',\n",
    "        'Internal Variation', 'Variation'\n",
    "    ],\n",
    "\n",
    "    '% VI': [\n",
    "        '% VI', 'Percentual VI', 'VI %', 'Percentual Variacao Interna',\n",
    "        'Perc VI', 'VI_Percent', 'Variation %'\n",
    "    ],\n",
    "\n",
    "    'LI Atual (%)': [\n",
    "        'LI Atual (%)', 'Limite Inferior', 'LI', 'Lim Inferior',\n",
    "        'Limite_Inferior', 'LIMITE_INF', 'LimInf', 'Lim_Inferior',\n",
    "        'Lower Limit', 'LL'\n",
    "    ],\n",
    "\n",
    "    'LS Atual (%)': [\n",
    "        'LS Atual (%)', 'Limite Superior', 'LS', 'Lim Superior',\n",
    "        'Limite_Superior', 'LIMITE_SUP', 'LimSup', 'Lim_Superior',\n",
    "        'Upper Limit', 'UL'\n",
    "    ],\n",
    "\n",
    "    'Período': [\n",
    "        'Período', 'Periodo', 'Data', 'Data Referencia',\n",
    "        'Mês/Ano', 'Mes Ano', 'Competencia', 'Period', 'Date'\n",
    "    ],\n",
    "\n",
    "    'Mês do exercício': [\n",
    "        'Mês do exercício', 'Mês', 'Mes', 'Mês Exercício',\n",
    "        'MesExercicio', 'MES', 'Mes_Exercicio', 'Mes_Ref', 'Month'\n",
    "    ],\n",
    "\n",
    "    'Ano do documento do material': [\n",
    "        'Ano do documento do material', 'Ano', 'Ano Documento',\n",
    "        'AnoDoc', 'ANO', 'Ano_Documento', 'Ano_Ref', 'Year'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(f\"   ✅ Dicionário de sinônimos: {len(DICIONARIO_SINONIMOS)} campos principais\")\n",
    "print(f\"   ✅ Total de variações mapeadas: {sum(len(v) for v in DICIONARIO_SINONIMOS.values())}\")\n",
    "print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 3: REGEX PATTERNS PARA DETECÇÃO DE CONTEÚDO\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🔍 SEÇÃO 1.3: Regex Patterns para Detecção de Conteúdo\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "PATTERNS_CONTEUDO = {\n",
    "    # CENTRO: 4 dígitos numéricos como string\n",
    "    'centro': {\n",
    "        'pattern': r'^\\d{4}$',\n",
    "        'description': 'Centro: 4 dígitos',\n",
    "        'exemplo': '1001'\n",
    "    },\n",
    "\n",
    "    # CÓDIGO DE MATERIAL/PRODUTO: formatos xx.xxx.xxx, x.xxx.xxx, xxxxxxxx, xxxxxxx\n",
    "    'codigo_material': {\n",
    "        'pattern': r'^(\\d{1,2}\\.\\d{3}\\.\\d{3}|\\d{7,8})$',\n",
    "        'description': 'Código Material: xx.xxx.xxx ou 7-8 dígitos',\n",
    "        'exemplo': '10.123.456'\n",
    "    },\n",
    "\n",
    "    # CÓDIGO DE GRUPO: pode ter texto com underscore ou números\n",
    "    'codigo_grupo': {\n",
    "        'pattern': r'^([A-Z0-9_]+|\\d{1,2}\\.\\d{3}\\.\\d{3}|\\d{7,8})$',\n",
    "        'description': 'Código Grupo: TEXTO_TEXTO ou numérico',\n",
    "        'exemplo': 'DIESEL_S10'\n",
    "    },\n",
    "\n",
    "    # TRANSPORTE/DOCUMENTO: 10 dígitos\n",
    "    'documento_transporte': {\n",
    "        'pattern': r'^\\d{10}$',\n",
    "        'description': 'Documento Transporte: 10 dígitos',\n",
    "        'exemplo': '1234567890'\n",
    "    },\n",
    "\n",
    "    # PERÍODO/DATA: diversos formatos\n",
    "    'data': {\n",
    "        'pattern': r'^\\d{2}/\\d{2}/\\d{4}$|^\\d{4}-\\d{2}-\\d{2}$|^\\d{1,2}\\.\\d{4}$|^\\d{2}\\.\\d{4}$',\n",
    "        'description': 'Data: dd/mm/yyyy, yyyy-mm-dd ou mm.yyyy',\n",
    "        'exemplo': '01/01/2025'\n",
    "    },\n",
    "\n",
    "    # VALORES NUMÉRICOS: com separadores regionais\n",
    "    'valor_numerico': {\n",
    "        'pattern': r'^-?\\d{1,3}(\\.\\d{3})*(,\\d+)?$|^-?\\d+(,\\d{3})*(\\.\\d+)?$|^-?\\d+(\\.\\d+)?$',\n",
    "        'description': 'Valor: 1.234,56 ou 1,234.56 ou 1234.56',\n",
    "        'exemplo': '1.234,56'\n",
    "    },\n",
    "\n",
    "    # PERCENTUAL\n",
    "    'percentual': {\n",
    "        'pattern': r'^-?\\d+([.,]\\d+)?%?$',\n",
    "        'description': 'Percentual: 12,34% ou 12.34',\n",
    "        'exemplo': '12,34%'\n",
    "    },\n",
    "\n",
    "    # SIGLA BASE: 2-4 letras maiúsculas\n",
    "    'sigla_base': {\n",
    "        'pattern': r'^[A-Z]{2,4}$',\n",
    "        'description': 'Sigla: 2-4 letras maiúsculas',\n",
    "        'exemplo': 'RLAM'\n",
    "    },\n",
    "\n",
    "    # TEXTO GERAL\n",
    "    'texto': {\n",
    "        'pattern': r'^[A-Za-zÀ-ÿ\\s\\-_/]+$',\n",
    "        'description': 'Texto: letras, espaços, hífen',\n",
    "        'exemplo': 'Gasolina Comum'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"   ✅ {len(PATTERNS_CONTEUDO)} patterns definidos\")\n",
    "for tipo, config in list(PATTERNS_CONTEUDO.items())[:3]:\n",
    "    print(f\"      • {tipo}: {config['description']}\")\n",
    "print(f\"      • ... e mais {len(PATTERNS_CONTEUDO) - 3} patterns\")\n",
    "print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 4: FUNÇÕES AUXILIARES - DETECÇÃO E VALIDAÇÃO\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"⚙️  SEÇÃO 1.4: Funções Auxiliares\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def normalizar_string(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza string para comparação (remove acentos, lowercase, apenas alfanuméricos).\n",
    "\n",
    "    Args:\n",
    "        s: String a normalizar\n",
    "\n",
    "    Returns:\n",
    "        String normalizada\n",
    "    \"\"\"\n",
    "    s = unicodedata.normalize('NFKD', str(s))\n",
    "    s = s.encode('ASCII', 'ignore').decode('ASCII')\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^a-z0-9]', '', s)\n",
    "    return s\n",
    "\n",
    "def calcular_similaridade(s1: str, s2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calcula similaridade entre duas strings (0.0 a 1.0).\n",
    "\n",
    "    Args:\n",
    "        s1: Primeira string\n",
    "        s2: Segunda string\n",
    "\n",
    "    Returns:\n",
    "        Score de similaridade\n",
    "    \"\"\"\n",
    "    from difflib import SequenceMatcher\n",
    "    s1_norm = normalizar_string(s1)\n",
    "    s2_norm = normalizar_string(s2)\n",
    "    return SequenceMatcher(None, s1_norm, s2_norm).ratio()\n",
    "\n",
    "def detectar_tipo_conteudo(valor: str, verbose: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Detecta tipo de conteúdo usando regex patterns.\n",
    "\n",
    "    Args:\n",
    "        valor: String a analisar\n",
    "        verbose: Se True, mostra detalhes\n",
    "\n",
    "    Returns:\n",
    "        Dict com tipo detectado e confiança\n",
    "    \"\"\"\n",
    "    if pd.isna(valor) or str(valor).strip() == '':\n",
    "        return {'tipo': 'VAZIO', 'confianca': 1.0, 'pattern': None}\n",
    "\n",
    "    valor_str = str(valor).strip()\n",
    "    matches = []\n",
    "\n",
    "    # Testar todos os patterns\n",
    "    for tipo, config in PATTERNS_CONTEUDO.items():\n",
    "        if re.match(config['pattern'], valor_str):\n",
    "            matches.append(tipo)\n",
    "            if verbose:\n",
    "                print(f\"   ✅ Match: {tipo} - {config['description']}\")\n",
    "\n",
    "    if not matches:\n",
    "        return {'tipo': 'DESCONHECIDO', 'confianca': 0.0, 'pattern': None}\n",
    "\n",
    "    # Priorizar mais específico\n",
    "    prioridade = ['centro', 'codigo_material', 'documento_transporte', 'data',\n",
    "                  'percentual', 'valor_numerico', 'sigla_base', 'codigo_grupo', 'texto']\n",
    "\n",
    "    for tipo_prior in prioridade:\n",
    "        if tipo_prior in matches:\n",
    "            return {\n",
    "                'tipo': tipo_prior.upper(),\n",
    "                'confianca': 1.0 if len(matches) == 1 else 0.8,\n",
    "                'pattern': PATTERNS_CONTEUDO[tipo_prior]['pattern'],\n",
    "                'alternativas': [m for m in matches if m != tipo_prior]\n",
    "            }\n",
    "\n",
    "    return {'tipo': matches[0].upper(), 'confianca': 0.7, 'pattern': PATTERNS_CONTEUDO[matches[0]]['pattern']}\n",
    "\n",
    "def analisar_coluna_completa(serie: pd.Series, amostra: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Analisa coluna completa para determinar tipo predominante.\n",
    "\n",
    "    Args:\n",
    "        serie: Série pandas\n",
    "        amostra: Número de valores a analisar\n",
    "\n",
    "    Returns:\n",
    "        Dict com análise completa\n",
    "    \"\"\"\n",
    "    valores_nao_nulos = serie.dropna().astype(str).head(amostra)\n",
    "\n",
    "    if len(valores_nao_nulos) == 0:\n",
    "        return {\n",
    "            'tipo_predominante': 'VAZIO',\n",
    "            'confianca': 0.0,\n",
    "            'tipos_encontrados': {},\n",
    "            'exemplos': []\n",
    "        }\n",
    "\n",
    "    tipos_contagem = {}\n",
    "    exemplos_por_tipo = {}\n",
    "\n",
    "    for valor in valores_nao_nulos:\n",
    "        deteccao = detectar_tipo_conteudo(valor)\n",
    "        tipo = deteccao['tipo']\n",
    "\n",
    "        tipos_contagem[tipo] = tipos_contagem.get(tipo, 0) + 1\n",
    "\n",
    "        if tipo not in exemplos_por_tipo:\n",
    "            exemplos_por_tipo[tipo] = []\n",
    "        if len(exemplos_por_tipo[tipo]) < 3:\n",
    "            exemplos_por_tipo[tipo].append(valor)\n",
    "\n",
    "    tipo_predominante = max(tipos_contagem, key=tipos_contagem.get)\n",
    "    total = len(valores_nao_nulos)\n",
    "    confianca = tipos_contagem[tipo_predominante] / total\n",
    "\n",
    "    return {\n",
    "        'tipo_predominante': tipo_predominante,\n",
    "        'confianca': confianca,\n",
    "        'tipos_encontrados': tipos_contagem,\n",
    "        'exemplos': exemplos_por_tipo.get(tipo_predominante, [])[:3],\n",
    "        'total_analisado': total\n",
    "    }\n",
    "\n",
    "def identificar_linha_cabecalho(df: pd.DataFrame, dicionario_termos: List[str]) -> int:\n",
    "    \"\"\"\n",
    "    Identifica linha de cabeçalho usando dicionário de termos conhecidos.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame bruto\n",
    "        dicionario_termos: Lista de termos que aparecem em cabeçalhos\n",
    "\n",
    "    Returns:\n",
    "        Índice da linha de cabeçalho (ou -1 se não encontrado)\n",
    "    \"\"\"\n",
    "    termos_norm = [normalizar_string(t) for t in dicionario_termos]\n",
    "\n",
    "    for idx in range(min(20, len(df))):\n",
    "        row = df.iloc[idx]\n",
    "        valores_norm = [normalizar_string(str(v)) for v in row if pd.notna(v)]\n",
    "\n",
    "        matches = sum(1 for termo in termos_norm if any(termo in v for v in valores_norm))\n",
    "        pct_match = matches / len(termos_norm) if termos_norm else 0\n",
    "\n",
    "        if pct_match >= 0.5:\n",
    "            return idx\n",
    "\n",
    "    return -1\n",
    "\n",
    "def validar_transformacao(df_antes: pd.DataFrame, df_depois: pd.DataFrame,\n",
    "                         nome_transformacao: str, permitir_reducao: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Valida transformação comparando estados antes/depois.\n",
    "\n",
    "    Args:\n",
    "        df_antes: DataFrame original\n",
    "        df_depois: DataFrame transformado\n",
    "        nome_transformacao: Nome da operação\n",
    "        permitir_reducao: Se True, permite redução de linhas/colunas\n",
    "\n",
    "    Returns:\n",
    "        Dict com resultado da validação\n",
    "    \"\"\"\n",
    "    resultado = {\n",
    "        'nome': nome_transformacao,\n",
    "        'passou': True,\n",
    "        'alertas': [],\n",
    "        'metricas': {}\n",
    "    }\n",
    "\n",
    "    # Linhas\n",
    "    linhas_antes = len(df_antes)\n",
    "    linhas_depois = len(df_depois)\n",
    "    diff_linhas = linhas_depois - linhas_antes\n",
    "\n",
    "    resultado['metricas']['linhas'] = {\n",
    "        'antes': linhas_antes,\n",
    "        'depois': linhas_depois,\n",
    "        'diferenca': diff_linhas\n",
    "    }\n",
    "\n",
    "    if diff_linhas < 0 and not permitir_reducao:\n",
    "        resultado['passou'] = False\n",
    "        resultado['alertas'].append(f\"❌ PERDA DE LINHAS: {abs(diff_linhas):,}\")\n",
    "\n",
    "    # Colunas\n",
    "    colunas_antes = len(df_antes.columns)\n",
    "    colunas_depois = len(df_depois.columns)\n",
    "\n",
    "    resultado['metricas']['colunas'] = {\n",
    "        'antes': colunas_antes,\n",
    "        'depois': colunas_depois,\n",
    "        'diferenca': colunas_depois - colunas_antes\n",
    "    }\n",
    "\n",
    "    # NULLs\n",
    "    nulls_antes = df_antes.isnull().sum().sum()\n",
    "    nulls_depois = df_depois.isnull().sum().sum()\n",
    "\n",
    "    resultado['metricas']['nulls'] = {\n",
    "        'antes': int(nulls_antes),\n",
    "        'depois': int(nulls_depois),\n",
    "        'diferenca': int(nulls_depois - nulls_antes)\n",
    "    }\n",
    "\n",
    "    return resultado\n",
    "\n",
    "def formatar_tabela_resumo(dados: Dict[str, Any], titulo: str = \"RESUMO\") -> None:\n",
    "    \"\"\"\n",
    "    Formata dicionário como tabela imprimível.\n",
    "\n",
    "    Args:\n",
    "        dados: Dicionário com dados\n",
    "        titulo: Título da tabela\n",
    "    \"\"\"\n",
    "    print(f\"\\n╔{'═' * 78}╗\")\n",
    "    print(f\"║ {titulo.center(76)} ║\")\n",
    "    print(f\"╠{'═' * 78}╣\")\n",
    "\n",
    "    for chave, valor in dados.items():\n",
    "        chave_fmt = chave.replace('_', ' ').title().ljust(35)\n",
    "\n",
    "        if isinstance(valor, (int, np.integer)):\n",
    "            valor_fmt = f\"{valor:,}\".rjust(40)\n",
    "        elif isinstance(valor, (float, np.floating)):\n",
    "            valor_fmt = f\"{valor:,.2f}\".rjust(40)\n",
    "        elif isinstance(valor, bool):\n",
    "            valor_fmt = (\"✅ SIM\" if valor else \"❌ NÃO\").rjust(40)\n",
    "        else:\n",
    "            valor_str = str(valor)\n",
    "            if len(valor_str) > 40:\n",
    "                valor_fmt = (valor_str[:37] + \"...\").rjust(40)\n",
    "            else:\n",
    "                valor_fmt = valor_str.rjust(40)\n",
    "\n",
    "        print(f\"║ {chave_fmt} : {valor_fmt} ║\")\n",
    "\n",
    "    print(f\"╚{'═' * 78}╝\")\n",
    "\n",
    "print(\"   ✅ normalizar_string()\")\n",
    "print(\"   ✅ calcular_similaridade()\")\n",
    "print(\"   ✅ detectar_tipo_conteudo()\")\n",
    "print(\"   ✅ analisar_coluna_completa()\")\n",
    "print(\"   ✅ identificar_linha_cabecalho()\")\n",
    "print(\"   ✅ validar_transformacao()\")\n",
    "print(\"   ✅ formatar_tabela_resumo()\")\n",
    "print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 5: SISTEMA UX/UI COM TIMER (PARTE 0.10)\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🎨 SEÇÃO 1.5: Sistema UX/UI com Timer e Memória\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "class DialogoPadraoAIVI:\n",
    "    \"\"\"\n",
    "    Classe base para diálogos interativos com:\n",
    "    - Memória de última escolha\n",
    "    - Timer de 10s com countdown visual\n",
    "    - Botões padronizados\n",
    "    - Logging transparente\n",
    "    \"\"\"\n",
    "\n",
    "    CONFIG_DIR = Path.home() / '.aivi_config'\n",
    "    TIMER_PADRAO = 10\n",
    "\n",
    "    def __init__(self, tipo: str, titulo: str, mensagem: str):\n",
    "        self.tipo = tipo\n",
    "        self.titulo = titulo\n",
    "        self.mensagem = mensagem\n",
    "        self.CONFIG_DIR.mkdir(exist_ok=True)\n",
    "        self.config_file = self.CONFIG_DIR / f'aivi_{tipo}_last.json'\n",
    "\n",
    "    def carregar_ultima_escolha(self) -> Optional[str]:\n",
    "        \"\"\"Carrega última escolha salva.\"\"\"\n",
    "        if self.config_file.exists():\n",
    "            try:\n",
    "                with open(self.config_file, 'r', encoding='utf-8') as f:\n",
    "                    config = json.load(f)\n",
    "                ultima = config.get('last_choice')\n",
    "\n",
    "                # Validar se é caminho e se existe\n",
    "                if self.tipo in ['diretorio', 'arquivo']:\n",
    "                    if ultima and Path(ultima).exists():\n",
    "                        return ultima\n",
    "                    return None\n",
    "                return ultima\n",
    "            except:\n",
    "                return None\n",
    "        return None\n",
    "\n",
    "    def salvar_escolha(self, escolha: str) -> None:\n",
    "        \"\"\"Salva escolha atual.\"\"\"\n",
    "        config = {\n",
    "            'last_choice': str(escolha),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'tipo': self.tipo\n",
    "        }\n",
    "        with open(self.config_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def selecionar_diretorio_trabalho() -> Path:\n",
    "    \"\"\"\n",
    "    Seleciona diretório de trabalho com UX/UI padrão:\n",
    "    - Memória de última pasta\n",
    "    - Timer de 10s\n",
    "    - Botões padronizados\n",
    "\n",
    "    Returns:\n",
    "        Path do diretório selecionado\n",
    "    \"\"\"\n",
    "    dialogo = DialogoPadraoAIVI(\n",
    "        'diretorio_trabalho',\n",
    "        'Diretório de Trabalho AIVI',\n",
    "        'Selecione a pasta onde os dados e outputs serão salvos'\n",
    "    )\n",
    "\n",
    "    ultimo_dir = dialogo.carregar_ultima_escolha()\n",
    "\n",
    "    # Criar janela\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Ocultar janela principal\n",
    "\n",
    "    # Se tem última pasta, criar janela com timer\n",
    "    if ultimo_dir:\n",
    "        root.deiconify()\n",
    "        root.title(\"AIVI - Diretório de Trabalho\")\n",
    "        root.geometry(\"600x300\")\n",
    "\n",
    "        # Centralizar\n",
    "        x = (root.winfo_screenwidth() // 2) - 300\n",
    "        y = (root.winfo_screenheight() // 2) - 150\n",
    "        root.geometry(f\"+{x}+{y}\")\n",
    "\n",
    "        resultado = {'path': None, 'cancelado': False, 'timeout': False}\n",
    "        contador = [10]\n",
    "\n",
    "        # Frame\n",
    "        frame = tk.Frame(root, padx=20, pady=20, bg='white')\n",
    "        frame.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "        # Título\n",
    "        tk.Label(\n",
    "            frame,\n",
    "            text=\"Diretório de Trabalho AIVI\",\n",
    "            font=('Arial', 14, 'bold'),\n",
    "            bg='white'\n",
    "        ).pack(pady=(0, 15))\n",
    "\n",
    "        # Mensagem\n",
    "        tk.Label(\n",
    "            frame,\n",
    "            text=f\"📂 Última pasta usada:\\n{ultimo_dir}\",\n",
    "            font=('Arial', 9),\n",
    "            bg='#E8F5E9',\n",
    "            fg='#2E7D32',\n",
    "            padx=10,\n",
    "            pady=10,\n",
    "            wraplength=550\n",
    "        ).pack(pady=(0, 10))\n",
    "\n",
    "        # Timer\n",
    "        label_timer = tk.Label(\n",
    "            frame,\n",
    "            text=f\"{contador[0]}s\",\n",
    "            font=('Arial', 20, 'bold'),\n",
    "            fg='#FF4444',\n",
    "            bg='white'\n",
    "        )\n",
    "        label_timer.pack(pady=(5, 20))\n",
    "\n",
    "        def countdown():\n",
    "            if contador[0] > 0 and not resultado['cancelado']:\n",
    "                contador[0] -= 1\n",
    "                label_timer.config(text=f\"{contador[0]}s\")\n",
    "                root.after(1000, countdown)\n",
    "            elif contador[0] == 0 and not resultado['cancelado']:\n",
    "                resultado['timeout'] = True\n",
    "                root.quit()\n",
    "                root.destroy()\n",
    "\n",
    "        # Botões\n",
    "        frame_btns = tk.Frame(frame, bg='white')\n",
    "        frame_btns.pack(side=tk.BOTTOM, pady=20)\n",
    "\n",
    "        def escolher_nova():\n",
    "            resultado['cancelado'] = True\n",
    "            root.withdraw()\n",
    "            diretorio = filedialog.askdirectory(\n",
    "                title=\"Diretório de Trabalho AIVI\",\n",
    "                initialdir=ultimo_dir\n",
    "            )\n",
    "            resultado['path'] = diretorio if diretorio else ultimo_dir\n",
    "            root.quit()\n",
    "            root.destroy()\n",
    "\n",
    "        def usar_ultima():\n",
    "            resultado['cancelado'] = True\n",
    "            resultado['path'] = ultimo_dir\n",
    "            root.quit()\n",
    "            root.destroy()\n",
    "\n",
    "        tk.Button(\n",
    "            frame_btns,\n",
    "            text=\"Escolher Nova Pasta\",\n",
    "            command=escolher_nova,\n",
    "            width=22,\n",
    "            height=2,\n",
    "            font=('Arial', 10, 'bold'),\n",
    "            bg='#4CAF50',\n",
    "            fg='white',\n",
    "            cursor='hand2'\n",
    "        ).pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        tk.Button(\n",
    "            frame_btns,\n",
    "            text=\"Usar Última Pasta\",\n",
    "            command=usar_ultima,\n",
    "            width=22,\n",
    "            height=2,\n",
    "            font=('Arial', 10),\n",
    "            bg='#2196F3',\n",
    "            fg='white',\n",
    "            cursor='hand2'\n",
    "        ).pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        # Iniciar timer\n",
    "        root.after(1000, countdown)\n",
    "        root.mainloop()\n",
    "\n",
    "        # Processar resultado\n",
    "        if resultado.get('timeout'):\n",
    "            print(f\"   ⏱️  Timeout (10s) - usando última pasta\")\n",
    "            resultado['path'] = ultimo_dir\n",
    "\n",
    "        if resultado['path']:\n",
    "            dialogo.salvar_escolha(resultado['path'])\n",
    "            return Path(resultado['path'])\n",
    "\n",
    "    # Sem última pasta ou escolher nova\n",
    "    root.withdraw()\n",
    "    diretorio = filedialog.askdirectory(title=\"Diretório de Trabalho AIVI\")\n",
    "\n",
    "    if diretorio:\n",
    "        dialogo.salvar_escolha(diretorio)\n",
    "        print(f\"   ✅ Diretório selecionado: {diretorio}\")\n",
    "        return Path(diretorio)\n",
    "    else:\n",
    "        raise ValueError(\"❌ Nenhum diretório selecionado\")\n",
    "\n",
    "print(\"   ✅ DialogoPadraoAIVI (classe base)\")\n",
    "print(\"   ✅ selecionar_diretorio_trabalho() (com timer + memória)\")\n",
    "print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 6: FILEMANAGER EXPANDIDO COM LOGS DE CAMPOS\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"📁 SEÇÃO 1.6: FileManager Expandido\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "class FileManagerAIVI:\n",
    "    \"\"\"\n",
    "    Gerenciador de arquivos e logs AIVI v4.1.\n",
    "\n",
    "    Funcionalidades:\n",
    "    - Estrutura de pastas completa\n",
    "    - Log de execução (geral + específico)\n",
    "    - Log de campos separado (PARTE 12.3)\n",
    "    - Pasta de exploração (PARTE 14)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, diretorio_base: Optional[Path] = None):\n",
    "        \"\"\"Inicializa FileManager.\"\"\"\n",
    "\n",
    "        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        print(f\"   🕐 Timestamp: {self.timestamp}\")\n",
    "\n",
    "        # Selecionar diretório base\n",
    "        if diretorio_base is None:\n",
    "            self.diretorio_base = selecionar_diretorio_trabalho()\n",
    "        else:\n",
    "            self.diretorio_base = Path(diretorio_base)\n",
    "\n",
    "        print(f\"   📂 Diretório base: {self.diretorio_base}\")\n",
    "\n",
    "        # Criar estrutura\n",
    "        self.diretorio_execucao = self.diretorio_base / f\"AIVI_DataIntegration_{self.timestamp}\"\n",
    "        self._criar_estrutura()\n",
    "        self._configurar_logging()\n",
    "\n",
    "        # Log inicial\n",
    "        self.logger.info(\"═\" * 80)\n",
    "        self.logger.info(\"NOVA EXECUÇÃO - AIVI DATA INTEGRATION v4.1\")\n",
    "        self.logger.info(\"═\" * 80)\n",
    "        self.logger.info(f\"Timestamp: {self.timestamp}\")\n",
    "        self.logger.info(f\"Diretório: {self.diretorio_execucao}\")\n",
    "        self.logger.info(f\"Python: {sys.version.split()[0]}\")\n",
    "        self.logger.info(f\"Pandas: {pd.__version__}\")\n",
    "\n",
    "        print()\n",
    "        print(f\"   ✅ FileManager inicializado\")\n",
    "        print(f\"   ✅ Sistema de logs ativo\")\n",
    "        print()\n",
    "\n",
    "    def _criar_estrutura(self):\n",
    "        \"\"\"Cria estrutura de pastas.\"\"\"\n",
    "\n",
    "        # Estrutura expandida v4.1\n",
    "        self.diretorios = {\n",
    "            'logs': self.diretorio_execucao / '01_Logs',\n",
    "            'logs_campos': self.diretorio_execucao / '01_Logs' / 'logs_campos',  # NOVO\n",
    "            'logs_execucao': self.diretorio_execucao / '01_Logs' / 'logs_execucao',  # NOVO\n",
    "            'dados_entrada': self.diretorio_execucao / '02_Dados_Entrada',\n",
    "            'dados_processados': self.diretorio_execucao / '03_Dados_Processados',\n",
    "            'dados_integrados': self.diretorio_execucao / '04_Dados_Integrados',\n",
    "            'exploracao': self.diretorio_execucao / '05_Exploracao',  # NOVO (PARTE 14)\n",
    "            'relatorios': self.diretorio_execucao / '06_Relatorios',\n",
    "            'validacoes': self.diretorio_execucao / '07_Validacoes',\n",
    "            'exports': self.diretorio_execucao / '08_Exports'\n",
    "        }\n",
    "\n",
    "        # Criar todas as pastas\n",
    "        for nome, caminho in self.diretorios.items():\n",
    "            caminho.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def _configurar_logging(self):\n",
    "        \"\"\"Configura sistema de logging.\"\"\"\n",
    "\n",
    "        log_file = self.diretorios['logs_execucao'] / f'aivi_analise_{self.timestamp}.log'\n",
    "\n",
    "        # Configurar logger\n",
    "        self.logger = logging.getLogger(f'AIVI_{self.timestamp}')\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "\n",
    "        # Handler para arquivo\n",
    "        fh = logging.FileHandler(log_file, encoding='utf-8')\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "\n",
    "        # Handler para console\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.INFO)\n",
    "\n",
    "        # Formato\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s | %(levelname)-8s | %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        fh.setFormatter(formatter)\n",
    "        ch.setFormatter(formatter)\n",
    "\n",
    "        self.logger.addHandler(fh)\n",
    "        self.logger.addHandler(ch)\n",
    "\n",
    "    def salvar_metadata(self, metadata_adicional: Dict = None) -> Path:\n",
    "        \"\"\"Salva metadata da execução.\"\"\"\n",
    "\n",
    "        metadata = {\n",
    "            'timestamp': self.timestamp,\n",
    "            'versao': CONSTANTES_AIVI['VERSAO_SISTEMA'],\n",
    "            'data_atualizacao': CONSTANTES_AIVI['DATA_ATUALIZACAO'],\n",
    "            'diretorio_base': str(self.diretorio_base),\n",
    "            'diretorio_execucao': str(self.diretorio_execucao),\n",
    "            'estrutura_pastas': {k: str(v) for k, v in self.diretorios.items()},\n",
    "            'python_version': sys.version,\n",
    "            'pandas_version': pd.__version__\n",
    "        }\n",
    "\n",
    "        if metadata_adicional:\n",
    "            metadata.update(metadata_adicional)\n",
    "\n",
    "        metadata_file = self.diretorios['logs'] / 'metadata.json'\n",
    "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        return metadata_file\n",
    "\n",
    "    def registrar_schema_campos(self, df: pd.DataFrame, fonte: str, arquivo_origem: str) -> Path:\n",
    "        \"\"\"\n",
    "        Registra schema completo de campos em log separado (PARTE 12.3).\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame a registrar\n",
    "            fonte: Nome da fonte (ex: 'SAP_YSMM_VI_ACOMP')\n",
    "            arquivo_origem: Nome do arquivo original\n",
    "\n",
    "        Returns:\n",
    "            Path do arquivo de log de campos\n",
    "        \"\"\"\n",
    "        schema_info = {\n",
    "            'fonte': fonte,\n",
    "            'arquivo_origem': arquivo_origem,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'total_registros': len(df),\n",
    "            'total_colunas': len(df.columns),\n",
    "            'campos': {}\n",
    "        }\n",
    "\n",
    "        # Analisar cada campo\n",
    "        for i, col in enumerate(df.columns, 1):\n",
    "            serie = df[col]\n",
    "\n",
    "            campo_info = {\n",
    "                'posicao': i,\n",
    "                'nome_original': col,\n",
    "                'tipo_pandas': str(serie.dtype),\n",
    "                'count_total': len(serie),\n",
    "                'count_nulos': int(serie.isnull().sum()),\n",
    "                'pct_nulos': float(serie.isnull().sum() / len(serie) * 100) if len(serie) > 0 else 0,\n",
    "                'count_unicos': int(serie.nunique()),\n",
    "                'pct_cardinalidade': float(serie.nunique() / len(serie) * 100) if len(serie) > 0 else 0\n",
    "            }\n",
    "\n",
    "            # Análise de conteúdo\n",
    "            analise = analisar_coluna_completa(serie, amostra=50)\n",
    "            campo_info['tipo_conteudo_detectado'] = analise['tipo_predominante']\n",
    "            campo_info['confianca_deteccao'] = analise['confianca']\n",
    "            campo_info['exemplos'] = analise['exemplos']\n",
    "\n",
    "            schema_info['campos'][col] = campo_info\n",
    "\n",
    "        # Salvar\n",
    "        log_campos_file = self.diretorios['logs_campos'] / f'{fonte}_{self.timestamp}_schema.json'\n",
    "        with open(log_campos_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(schema_info, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        self.logger.info(f\"Schema de campos registrado: {log_campos_file.name}\")\n",
    "\n",
    "        return log_campos_file\n",
    "\n",
    "print(\"   ✅ FileManagerAIVI (classe expandida)\")\n",
    "print(\"   ✅ Suporte a logs de campos (PARTE 12.3)\")\n",
    "print(\"   ✅ Pasta de exploração (PARTE 14)\")\n",
    "print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 7: INICIALIZAÇÃO E TESTE\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🚀 SEÇÃO 1.7: Inicializando FileManager\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "\n",
    "# Inicializar (vai abrir GUI)\n",
    "try:\n",
    "    fm = FileManagerAIVI()\n",
    "\n",
    "    # Criar aliases para compatibilidade\n",
    "    gerenciador = fm\n",
    "    gp = fm\n",
    "\n",
    "    # Aliases de diretórios\n",
    "    DIR_LOGS = fm.diretorios['logs']\n",
    "    DIR_LOGS_CAMPOS = fm.diretorios['logs_campos']\n",
    "    DIR_LOGS_EXECUCAO = fm.diretorios['logs_execucao']\n",
    "    DIR_DADOS_ENTRADA = fm.diretorios['dados_entrada']\n",
    "    DIR_DADOS_PROCESSADOS = fm.diretorios['dados_processados']\n",
    "    DIR_DADOS_INTEGRADOS = fm.diretorios['dados_integrados']\n",
    "    DIR_EXPLORACAO = fm.diretorios['exploracao']\n",
    "    DIR_RELATORIOS = fm.diretorios['relatorios']\n",
    "    DIR_VALIDACOES = fm.diretorios['validacoes']\n",
    "    DIR_EXPORTS = fm.diretorios['exports']\n",
    "\n",
    "    # Salvar metadata inicial\n",
    "    metadata_file = fm.salvar_metadata({\n",
    "        'bloco': 1,\n",
    "        'fase': 'configuracao_completa',\n",
    "        'modulos_carregados': modulos_carregados,\n",
    "        'constantes': CONSTANTES_AIVI,\n",
    "        'patterns_definidos': len(PATTERNS_CONTEUDO),\n",
    "        'sinonimos_mapeados': sum(len(v) for v in DICIONARIO_SINONIMOS.values())\n",
    "    })\n",
    "\n",
    "    print()\n",
    "\n",
    "except Exception as e:\n",
    "    print()\n",
    "    print(\"╔\" + \"═\" * 78 + \"╗\")\n",
    "    print(\"║\" + \" ❌ ERRO NA INICIALIZAÇÃO \".center(78) + \"║\")\n",
    "    print(\"╚\" + \"═\" * 78 + \"╝\")\n",
    "    print()\n",
    "    print(f\"Erro: {str(e)}\")\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n",
    "    raise\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 8: INTEGRAÇÃO COM DICIONÁRIO PERSISTENTE (PROCESSAR ARQUIVOS)\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"📚 SEÇÃO 1.8: Integração com Dicionário Persistente\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Locais padrão onde o dicionário pode estar\n",
    "LOCAIS_DICIONARIO = [\n",
    "    # Local 1: Mesma pasta do notebook\n",
    "    Path.cwd() / 'DICT_Dicionario_Persistente.json',\n",
    "\n",
    "    # Local 2: Pasta AIVI base\n",
    "    Path(\"E:/OneDrive - VIBRA/NMCV - Documentos/Indicador/AIVI\") / 'DICT_Dicionario_Persistente.json',\n",
    "\n",
    "    # Local 3: Dentro da execução atual\n",
    "    fm.diretorio_execucao / '08_Dicionarios' / 'DICT_Dicionario_Persistente.json',\n",
    "\n",
    "    # Local 4: Pasta compartilhada (se existir)\n",
    "    Path(\"E:/OneDrive - VIBRA/NMCV - Documentos/Indicador/AIVI/_DICIONARIOS\") / 'DICT_Dicionario_Persistente.json'\n",
    "]\n",
    "\n",
    "def carregar_dicionario_persistente() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Carrega dicionário persistente criado por PROCESSAR ARQUIVOS DESCONHECIDOS.\n",
    "\n",
    "    Returns:\n",
    "        Dict com dicionário completo ou dict vazio se não encontrado\n",
    "    \"\"\"\n",
    "    print(\"\\n   🔍 Buscando dicionário persistente...\")\n",
    "\n",
    "    for i, local in enumerate(LOCAIS_DICIONARIO, 1):\n",
    "        if local.exists():\n",
    "            print(f\"   ✅ Encontrado em: {local}\")\n",
    "\n",
    "            try:\n",
    "                with open(local, 'r', encoding='utf-8') as f:\n",
    "                    dicionario = json.load(f)\n",
    "\n",
    "                # Validar estrutura\n",
    "                if 'arquivos' in dicionario and 'ultima_atualizacao' in dicionario:\n",
    "                    print(f\"      Arquivos registrados: {len(dicionario['arquivos'])}\")\n",
    "                    print(f\"      Última atualização: {dicionario['ultima_atualizacao']}\")\n",
    "\n",
    "                    # Salvar referência global\n",
    "                    fm.logger.info(f\"Dicionário persistente carregado: {local}\")\n",
    "\n",
    "                    return dicionario\n",
    "                else:\n",
    "                    print(f\"      ⚠️  Estrutura inválida, pulando...\")\n",
    "            except Exception as e:\n",
    "                print(f\"      ❌ Erro ao carregar: {e}\")\n",
    "\n",
    "    print(\"   ℹ️  Dicionário não encontrado - será criado quando necessário\")\n",
    "\n",
    "    # Retornar estrutura vazia\n",
    "    return {\n",
    "        'arquivos': {},\n",
    "        'ultima_atualizacao': None,\n",
    "        'versao': '1.0'\n",
    "    }\n",
    "\n",
    "def consultar_tratamento_arquivo(nome_arquivo: str, dicionario: Dict) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Consulta tratamento conhecido para um arquivo.\n",
    "\n",
    "    Args:\n",
    "        nome_arquivo: Nome do arquivo (ex: '2025-2024-YSMM_VI_ACOMP.xlsx')\n",
    "        dicionario: Dicionário persistente\n",
    "\n",
    "    Returns:\n",
    "        Dict com tratamento ou None se não encontrado\n",
    "    \"\"\"\n",
    "    # Normalizar nome\n",
    "    nome_norm = normalizar_string(nome_arquivo)\n",
    "\n",
    "    # Buscar por nome exato\n",
    "    if nome_arquivo in dicionario['arquivos']:\n",
    "        return dicionario['arquivos'][nome_arquivo]\n",
    "\n",
    "    # Buscar por nome normalizado\n",
    "    for arq_conhecido, tratamento in dicionario['arquivos'].items():\n",
    "        if normalizar_string(arq_conhecido) == nome_norm:\n",
    "            print(f\"   ✅ Tratamento encontrado: {arq_conhecido}\")\n",
    "            return tratamento\n",
    "\n",
    "    # Buscar por similaridade (≥80%)\n",
    "    for arq_conhecido, tratamento in dicionario['arquivos'].items():\n",
    "        similaridade = calcular_similaridade(nome_arquivo, arq_conhecido)\n",
    "        if similaridade >= 0.80:\n",
    "            print(f\"   ⚠️  Tratamento similar encontrado ({similaridade:.0%}): {arq_conhecido}\")\n",
    "            print(f\"      Confirme se é o mesmo tipo de arquivo\")\n",
    "            return tratamento\n",
    "\n",
    "    return None\n",
    "\n",
    "def registrar_novo_tratamento(nome_arquivo: str, tratamento: Dict,\n",
    "                             dicionario: Dict, salvar_em: Path) -> None:\n",
    "    \"\"\"\n",
    "    Registra novo tratamento no dicionário persistente.\n",
    "\n",
    "    Args:\n",
    "        nome_arquivo: Nome do arquivo\n",
    "        tratamento: Dict com informações do tratamento\n",
    "        dicionario: Dicionário atual\n",
    "        salvar_em: Path onde salvar o dicionário atualizado\n",
    "    \"\"\"\n",
    "    # Adicionar ao dicionário\n",
    "    dicionario['arquivos'][nome_arquivo] = tratamento\n",
    "    dicionario['ultima_atualizacao'] = datetime.now().isoformat()\n",
    "\n",
    "    # Salvar\n",
    "    salvar_em.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(salvar_em, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dicionario, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"   💾 Dicionário atualizado: {salvar_em.name}\")\n",
    "\n",
    "    fm.logger.info(f\"Novo tratamento registrado: {nome_arquivo}\")\n",
    "\n",
    "def chamar_processador_arquivos(arquivo_path: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Chama notebook PROCESSAR ARQUIVOS DESCONHECIDOS (simula funcionalidade).\n",
    "\n",
    "    NOTA: Esta é uma INTERFACE. O processamento real deve ser feito\n",
    "    manualmente no notebook PROCESSAR ARQUIVOS DESCONHECIDOS v3.ipynb\n",
    "\n",
    "    Args:\n",
    "        arquivo_path: Path do arquivo a processar\n",
    "\n",
    "    Returns:\n",
    "        Dict com informações do tratamento\n",
    "    \"\"\"\n",
    "    print(f\"\\n   ⚠️  NOVO ARQUIVO DETECTADO: {arquivo_path.name}\")\n",
    "    print(\"   \" + \"─\" * 70)\n",
    "    print(\"   Este arquivo NÃO está no dicionário persistente.\")\n",
    "    print()\n",
    "    print(\"   📋 AÇÕES NECESSÁRIAS:\")\n",
    "    print(\"   1. Abra o notebook: PROCESSAR ARQUIVOS DESCONHECIDOS v3.ipynb\")\n",
    "    print(\"   2. Execute o processamento deste arquivo\")\n",
    "    print(\"   3. O dicionário será atualizado automaticamente\")\n",
    "    print(\"   4. Retorne aqui e execute novamente\")\n",
    "    print()\n",
    "    print(\"   💡 ALTERNATIVA RÁPIDA:\")\n",
    "    print(\"   Se você sabe o tratamento, pode informar manualmente:\")\n",
    "\n",
    "    # Coletar informações manualmente (interface simplificada)\n",
    "    resposta = input(\"\\n   Deseja informar tratamento MANUALMENTE? (S/N): \").upper()\n",
    "\n",
    "    if resposta == 'S':\n",
    "        print(\"\\n   📝 Informações do tratamento:\")\n",
    "\n",
    "        sheet_nome = input(\"      Nome da aba (sheet): \").strip() or None\n",
    "        linha_cabecalho = input(\"      Linha do cabeçalho (0-indexed): \").strip()\n",
    "        linha_cabecalho = int(linha_cabecalho) if linha_cabecalho else 0\n",
    "\n",
    "        colunas_remover = input(\"      Colunas a remover (separadas por vírgula): \").strip()\n",
    "        colunas_remover = [c.strip() for c in colunas_remover.split(',') if c.strip()]\n",
    "\n",
    "        linhas_remover = input(\"      Linhas a remover ANTES do cabeçalho (sep. vírgula): \").strip()\n",
    "        linhas_remover = [int(l.strip()) for l in linhas_remover.split(',') if l.strip()]\n",
    "\n",
    "        tratamento = {\n",
    "            'arquivo': arquivo_path.name,\n",
    "            'sheet_nome': sheet_nome,\n",
    "            'linha_cabecalho': linha_cabecalho,\n",
    "            'colunas_remover': colunas_remover,\n",
    "            'linhas_remover_antes_cabecalho': linhas_remover,\n",
    "            'data_criacao': datetime.now().isoformat(),\n",
    "            'metodo': 'MANUAL'\n",
    "        }\n",
    "\n",
    "        print(\"\\n   ✅ Tratamento registrado!\")\n",
    "        return tratamento\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"❌ Arquivo não processado: {arquivo_path.name}\\n\"\n",
    "            f\"   Execute PROCESSAR ARQUIVOS DESCONHECIDOS v3.ipynb primeiro\"\n",
    "        )\n",
    "\n",
    "# Carregar dicionário\n",
    "try:\n",
    "    DICIONARIO_PERSISTENTE = carregar_dicionario_persistente()\n",
    "    print()\n",
    "    print(\"   ✅ Sistema de dicionário persistente: ATIVO\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n   ⚠️  Erro ao carregar dicionário: {e}\")\n",
    "    print(\"   ℹ️  Continuando com dicionário vazio\")\n",
    "    DICIONARIO_PERSISTENTE = {'arquivos': {}, 'ultima_atualizacao': None, 'versao': '1.0'}\n",
    "\n",
    "print()\n",
    "\n",
    "# Atualizar resumo final\n",
    "print(\"╔\" + \"═\" * 78 + \"╗\")\n",
    "print(\"║\" + \" ✅ BLOCO 1 COMPLETO - COM INTEGRAÇÃO DICIONÁRIO \".center(78) + \"║\")\n",
    "print(\"╚\" + \"═\" * 78 + \"╝\")\n",
    "print()\n",
    "\n",
    "formatar_tabela_resumo({\n",
    "    'timestamp': fm.timestamp,\n",
    "    'versao_sistema': CONSTANTES_AIVI['VERSAO_SISTEMA'],\n",
    "    'diretorio_execucao': fm.diretorio_execucao.name,\n",
    "    'total_pastas_criadas': len(fm.diretorios),\n",
    "    'dicionario_persistente': 'INTEGRADO' if DICIONARIO_PERSISTENTE['arquivos'] else 'VAZIO',\n",
    "    'arquivos_conhecidos': len(DICIONARIO_PERSISTENTE['arquivos']),\n",
    "    'sistema_completo': 'PRONTO PARA USO'\n",
    "}, \"RESUMO FINAL - BLOCO 1\")\n",
    "\n",
    "print()\n",
    "print(\"📋 VARIÁVEIS GLOBAIS ADICIONADAS:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   • DICIONARIO_PERSISTENTE (dict)\")\n",
    "print(\"   • LOCAIS_DICIONARIO (list)\")\n",
    "print(\"   • Funções:\")\n",
    "print(\"     - carregar_dicionario_persistente()\")\n",
    "print(\"     - consultar_tratamento_arquivo()\")\n",
    "print(\"     - registrar_novo_tratamento()\")\n",
    "print(\"     - chamar_processador_arquivos()\")\n",
    "\n",
    "print()\n",
    "print(\"✅ BLOCO 1 OK - CONFIRME PARA PROSSEGUIR PARA BLOCO 2\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# RESULTADO FINAL DO BLOCO 1\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"╔\" + \"═\" * 78 + \"╗\")\n",
    "print(\"║\" + \" ✅ BLOCO 1 CONCLUÍDO COM SUCESSO \".center(78) + \"║\")\n",
    "print(\"╚\" + \"═\" * 78 + \"╝\")\n",
    "print()\n",
    "\n",
    "# Resumo em tabela formatada\n",
    "formatar_tabela_resumo({\n",
    "    'timestamp': fm.timestamp,\n",
    "    'versao_sistema': CONSTANTES_AIVI['VERSAO_SISTEMA'],\n",
    "    'diretorio_execucao': fm.diretorio_execucao.name,\n",
    "    'total_pastas_criadas': len(fm.diretorios),\n",
    "    'constantes_aivi': len(CONSTANTES_AIVI),\n",
    "    'campos_obrigatorios': len(SCHEMA_AIVI_CAMPOS_OBRIGATORIOS),\n",
    "    'patterns_regex': len(PATTERNS_CONTEUDO),\n",
    "    'sinonimos_mapeados': sum(len(v) for v in DICIONARIO_SINONIMOS.values()),\n",
    "    'funcoes_auxiliares': 7,\n",
    "    'sistema_ux_ui': 'ATIVO (timer + memória)',\n",
    "    'log_campos': 'ATIVO (PARTE 12.3)',\n",
    "    'exploracao': 'ATIVO (PARTE 14)'\n",
    "}, \"RESUMO DO BLOCO 1\")\n",
    "\n",
    "print()\n",
    "print(\"📂 ESTRUTURA DE PASTAS CRIADA:\")\n",
    "print(\"-\" * 80)\n",
    "for nome, caminho in fm.diretorios.items():\n",
    "    print(f\"   {nome.ljust(20)} → {caminho.name}\")\n",
    "\n",
    "print()\n",
    "print(\"🔗 VARIÁVEIS GLOBAIS DISPONÍVEIS:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   • fm / gerenciador / gp (FileManager)\")\n",
    "print(\"   • DIR_LOGS, DIR_LOGS_CAMPOS, DIR_DADOS_ENTRADA, etc\")\n",
    "print(\"   • CONSTANTES_AIVI (dict)\")\n",
    "print(\"   • SCHEMA_AIVI_CAMPOS_OBRIGATORIOS (list)\")\n",
    "print(\"   • DICIONARIO_SINONIMOS (dict)\")\n",
    "print(\"   • PATTERNS_CONTEUDO (dict)\")\n",
    "print(\"   • Funções: normalizar_string(), detectar_tipo_conteudo(), etc\")\n",
    "\n",
    "print()\n",
    "print(\"📋 PRÓXIMOS PASSOS:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   1. ✅ Verifique se o diretório foi criado corretamente\")\n",
    "print(\"   2. ✅ Confirme que todas as pastas existem (9 pastas)\")\n",
    "print(\"   3. ✅ Abra o arquivo de log em 01_Logs/logs_execucao/\")\n",
    "print(\"   4. 📤 Envie: 'BLOCO 1 OK' para prosseguir\")\n",
    "print(\"   5. ⏭️  BLOCO 2 será o próximo (Carregador de Arquivos)\")\n",
    "\n",
    "print()\n",
    "print(\"🔗 CAMINHO COMPLETO:\")\n",
    "print(f\"   {fm.diretorio_execucao}\")\n",
    "print()\n",
    "print(\"═\" * 80)"
   ],
   "id": "195b3fbf337a7cfa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔══════════════════════════════════════════════════════════════════════════════╗\n",
      "║                 🚀 BLOCO 1: CONFIGURAÇÃO COMPLETA DO AMBIENTE                 ║\n",
      "╚══════════════════════════════════════════════════════════════════════════════╝\n",
      "\n",
      "📦 SEÇÃO 1.1: Imports\n",
      "--------------------------------------------------------------------------------\n",
      "   ✅ Bibliotecas padrão: sys, os, json, logging, datetime\n",
      "   ✅ Análise de dados: pandas, numpy\n",
      "   ✅ GUI: tkinter\n",
      "   ✅ Configurações aplicadas\n",
      "\n",
      "📋 Versões:\n",
      "   • Python: 3.11.9\n",
      "   • Pandas: 2.3.3\n",
      "   • NumPy: 2.3.3\n",
      "\n",
      "📊 SEÇÃO 1.2: Constantes AIVI 2025\n",
      "--------------------------------------------------------------------------------\n",
      "   ✅ Ano de vigência: 2025\n",
      "   ✅ Versão do sistema: 4.1\n",
      "   ✅ 14 constantes definidas\n",
      "\n",
      "   ✅ Schema AIVI: 9 campos obrigatórios\n",
      "\n",
      "   ✅ Dicionário de sinônimos: 11 campos principais\n",
      "   ✅ Total de variações mapeadas: 111\n",
      "\n",
      "🔍 SEÇÃO 1.3: Regex Patterns para Detecção de Conteúdo\n",
      "--------------------------------------------------------------------------------\n",
      "   ✅ 9 patterns definidos\n",
      "      • centro: Centro: 4 dígitos\n",
      "      • codigo_material: Código Material: xx.xxx.xxx ou 7-8 dígitos\n",
      "      • codigo_grupo: Código Grupo: TEXTO_TEXTO ou numérico\n",
      "      • ... e mais 6 patterns\n",
      "\n",
      "⚙️  SEÇÃO 1.4: Funções Auxiliares\n",
      "--------------------------------------------------------------------------------\n",
      "   ✅ normalizar_string()\n",
      "   ✅ calcular_similaridade()\n",
      "   ✅ detectar_tipo_conteudo()\n",
      "   ✅ analisar_coluna_completa()\n",
      "   ✅ identificar_linha_cabecalho()\n",
      "   ✅ validar_transformacao()\n",
      "   ✅ formatar_tabela_resumo()\n",
      "\n",
      "🎨 SEÇÃO 1.5: Sistema UX/UI com Timer e Memória\n",
      "--------------------------------------------------------------------------------\n",
      "   ✅ DialogoPadraoAIVI (classe base)\n",
      "   ✅ selecionar_diretorio_trabalho() (com timer + memória)\n",
      "\n",
      "📁 SEÇÃO 1.6: FileManager Expandido\n",
      "--------------------------------------------------------------------------------\n",
      "   ✅ FileManagerAIVI (classe expandida)\n",
      "   ✅ Suporte a logs de campos (PARTE 12.3)\n",
      "   ✅ Pasta de exploração (PARTE 14)\n",
      "\n",
      "🚀 SEÇÃO 1.7: Inicializando FileManager\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "   🕐 Timestamp: 20251018_225227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 22:52:29 | INFO     | ════════════════════════════════════════════════════════════════════════════════\n",
      "2025-10-18 22:52:29 | INFO     | NOVA EXECUÇÃO - AIVI DATA INTEGRATION v4.1\n",
      "2025-10-18 22:52:29 | INFO     | ════════════════════════════════════════════════════════════════════════════════\n",
      "2025-10-18 22:52:29 | INFO     | Timestamp: 20251018_225227\n",
      "2025-10-18 22:52:29 | INFO     | Diretório: E:\\OneDrive - VIBRA\\NMCV - Documentos\\Indicador\\AIVI\\AIVI-INTEGRAÇÃO\\AIVI_DataIntegration_20251018_225227\n",
      "2025-10-18 22:52:29 | INFO     | Python: 3.11.9\n",
      "2025-10-18 22:52:29 | INFO     | Pandas: 2.3.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📂 Diretório base: E:\\OneDrive - VIBRA\\NMCV - Documentos\\Indicador\\AIVI\\AIVI-INTEGRAÇÃO\n",
      "\n",
      "   ✅ FileManager inicializado\n",
      "   ✅ Sistema de logs ativo\n",
      "\n",
      "\n",
      "📚 SEÇÃO 1.8: Integração com Dicionário Persistente\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "   🔍 Buscando dicionário persistente...\n",
      "   ℹ️  Dicionário não encontrado - será criado quando necessário\n",
      "\n",
      "   ✅ Sistema de dicionário persistente: ATIVO\n",
      "\n",
      "╔══════════════════════════════════════════════════════════════════════════════╗\n",
      "║                ✅ BLOCO 1 COMPLETO - COM INTEGRAÇÃO DICIONÁRIO                ║\n",
      "╚══════════════════════════════════════════════════════════════════════════════╝\n",
      "\n",
      "\n",
      "╔══════════════════════════════════════════════════════════════════════════════╗\n",
      "║                            RESUMO FINAL - BLOCO 1                            ║\n",
      "╠══════════════════════════════════════════════════════════════════════════════╣\n",
      "║ Timestamp                           :                          20251018_225227 ║\n",
      "║ Versao Sistema                      :                                      4.1 ║\n",
      "║ Diretorio Execucao                  :     AIVI_DataIntegration_20251018_225227 ║\n",
      "║ Total Pastas Criadas                :                                       10 ║\n",
      "║ Dicionario Persistente              :                                    VAZIO ║\n",
      "║ Arquivos Conhecidos                 :                                        0 ║\n",
      "║ Sistema Completo                    :                          PRONTO PARA USO ║\n",
      "╚══════════════════════════════════════════════════════════════════════════════╝\n",
      "\n",
      "📋 VARIÁVEIS GLOBAIS ADICIONADAS:\n",
      "--------------------------------------------------------------------------------\n",
      "   • DICIONARIO_PERSISTENTE (dict)\n",
      "   • LOCAIS_DICIONARIO (list)\n",
      "   • Funções:\n",
      "     - carregar_dicionario_persistente()\n",
      "     - consultar_tratamento_arquivo()\n",
      "     - registrar_novo_tratamento()\n",
      "     - chamar_processador_arquivos()\n",
      "\n",
      "✅ BLOCO 1 OK - CONFIRME PARA PROSSEGUIR PARA BLOCO 2\n",
      "================================================================================\n",
      "╔══════════════════════════════════════════════════════════════════════════════╗\n",
      "║                       ✅ BLOCO 1 CONCLUÍDO COM SUCESSO                        ║\n",
      "╚══════════════════════════════════════════════════════════════════════════════╝\n",
      "\n",
      "\n",
      "╔══════════════════════════════════════════════════════════════════════════════╗\n",
      "║                              RESUMO DO BLOCO 1                               ║\n",
      "╠══════════════════════════════════════════════════════════════════════════════╣\n",
      "║ Timestamp                           :                          20251018_225227 ║\n",
      "║ Versao Sistema                      :                                      4.1 ║\n",
      "║ Diretorio Execucao                  :     AIVI_DataIntegration_20251018_225227 ║\n",
      "║ Total Pastas Criadas                :                                       10 ║\n",
      "║ Constantes Aivi                     :                                       14 ║\n",
      "║ Campos Obrigatorios                 :                                        9 ║\n",
      "║ Patterns Regex                      :                                        9 ║\n",
      "║ Sinonimos Mapeados                  :                                      111 ║\n",
      "║ Funcoes Auxiliares                  :                                        7 ║\n",
      "║ Sistema Ux Ui                       :                  ATIVO (timer + memória) ║\n",
      "║ Log Campos                          :                       ATIVO (PARTE 12.3) ║\n",
      "║ Exploracao                          :                         ATIVO (PARTE 14) ║\n",
      "╚══════════════════════════════════════════════════════════════════════════════╝\n",
      "\n",
      "📂 ESTRUTURA DE PASTAS CRIADA:\n",
      "--------------------------------------------------------------------------------\n",
      "   logs                 → 01_Logs\n",
      "   logs_campos          → logs_campos\n",
      "   logs_execucao        → logs_execucao\n",
      "   dados_entrada        → 02_Dados_Entrada\n",
      "   dados_processados    → 03_Dados_Processados\n",
      "   dados_integrados     → 04_Dados_Integrados\n",
      "   exploracao           → 05_Exploracao\n",
      "   relatorios           → 06_Relatorios\n",
      "   validacoes           → 07_Validacoes\n",
      "   exports              → 08_Exports\n",
      "\n",
      "🔗 VARIÁVEIS GLOBAIS DISPONÍVEIS:\n",
      "--------------------------------------------------------------------------------\n",
      "   • fm / gerenciador / gp (FileManager)\n",
      "   • DIR_LOGS, DIR_LOGS_CAMPOS, DIR_DADOS_ENTRADA, etc\n",
      "   • CONSTANTES_AIVI (dict)\n",
      "   • SCHEMA_AIVI_CAMPOS_OBRIGATORIOS (list)\n",
      "   • DICIONARIO_SINONIMOS (dict)\n",
      "   • PATTERNS_CONTEUDO (dict)\n",
      "   • Funções: normalizar_string(), detectar_tipo_conteudo(), etc\n",
      "\n",
      "📋 PRÓXIMOS PASSOS:\n",
      "--------------------------------------------------------------------------------\n",
      "   1. ✅ Verifique se o diretório foi criado corretamente\n",
      "   2. ✅ Confirme que todas as pastas existem (9 pastas)\n",
      "   3. ✅ Abra o arquivo de log em 01_Logs/logs_execucao/\n",
      "   4. 📤 Envie: 'BLOCO 1 OK' para prosseguir\n",
      "   5. ⏭️  BLOCO 2 será o próximo (Carregador de Arquivos)\n",
      "\n",
      "🔗 CAMINHO COMPLETO:\n",
      "   E:\\OneDrive - VIBRA\\NMCV - Documentos\\Indicador\\AIVI\\AIVI-INTEGRAÇÃO\\AIVI_DataIntegration_20251018_225227\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T14:51:08.815576Z",
     "start_time": "2025-10-17T14:51:08.765041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                 BLOCO 2: DICIONÁRIO DE DADOS PERSISTENTE                     ║\n",
    "║                    Sistema Auto-Atualizável de Schemas                       ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║  OBJETIVO:                                                                   ║\n",
    "║    • Carregar dicionários persistentes existentes                            ║\n",
    "║    • Criar sistema de referência (não hardcode)                              ║\n",
    "║    • Permitir processamento de arquivos novos com detecção automática        ║\n",
    "║    • Integrar com FileManager do BLOCO 1                                     ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║  RENOMEAÇÃO:                                                                 ║\n",
    "║    • AIVI → ETL-Integração (conforme solicitado)                             ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 2.1: CLASSE DICIONÁRIO DE DADOS PERSISTENTE\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"╔\" + \"═\" * 78 + \"╗\")\n",
    "print(\"║\" + \" 📚 BLOCO 2: DICIONÁRIO DE DADOS PERSISTENTE\".center(78) + \"║\")\n",
    "print(\"╚\" + \"═\" * 78 + \"╝\")\n",
    "print()\n",
    "\n",
    "print(\"📖 SEÇÃO 2.1: Classe Dicionário Persistente\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "class DicionarioDadosPersistente:\n",
    "    \"\"\"\n",
    "    Gerencia dicionários de dados persistentes e auto-atualizáveis.\n",
    "\n",
    "    Funcionalidades:\n",
    "    - Carrega dicionários existentes de arquivos DICT_*.xlsx\n",
    "    - Registra novos schemas detectados\n",
    "    - Mapeia campos automaticamente\n",
    "    - Gera código de reprodução\n",
    "    - Integra com FileManager\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filemanager: FileManagerAIVI):\n",
    "        \"\"\"\n",
    "        Inicializa DicionarioDadosPersistente.\n",
    "\n",
    "        Args:\n",
    "            filemanager: Instância do FileManager\n",
    "        \"\"\"\n",
    "        self.fm = filemanager\n",
    "        self.pasta_dicionarios = self.fm.diretorio_base / 'dicionarios_persistentes'\n",
    "        self.pasta_dicionarios.mkdir(exist_ok=True)\n",
    "\n",
    "        # Estrutura de dados\n",
    "        self.dicionarios_carregados = {}\n",
    "        self.schemas_conhecidos = {}\n",
    "        self.historico_deteccoes = []\n",
    "\n",
    "        print(f\"   ✅ Pasta de dicionários: {self.pasta_dicionarios.name}\")\n",
    "        print()\n",
    "\n",
    "    def buscar_dicionarios_existentes(self) -> List[Path]:\n",
    "        \"\"\"\n",
    "        Busca todos os arquivos DICT_*.xlsx na pasta.\n",
    "\n",
    "        Returns:\n",
    "            Lista de Paths dos dicionários encontrados\n",
    "        \"\"\"\n",
    "        dicionarios = list(self.pasta_dicionarios.glob('DICT_*.xlsx'))\n",
    "\n",
    "        print(f\"🔍 Buscando dicionários existentes...\")\n",
    "        print(f\"   Encontrados: {len(dicionarios)} arquivo(s)\")\n",
    "\n",
    "        for dict_file in dicionarios:\n",
    "            print(f\"   • {dict_file.name}\")\n",
    "\n",
    "        return dicionarios\n",
    "\n",
    "    def carregar_dicionario(self, arquivo: Path) -> Dict:\n",
    "        \"\"\"\n",
    "        Carrega dicionário de arquivo Excel.\n",
    "\n",
    "        Args:\n",
    "            arquivo: Path do arquivo DICT_*.xlsx\n",
    "\n",
    "        Returns:\n",
    "            Dict com schema carregado\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extrair nome da fonte do nome do arquivo\n",
    "            # Formato: DICT_<fonte>_<timestamp>.xlsx\n",
    "            nome_base = arquivo.stem  # Remove extensão\n",
    "            partes = nome_base.split('_')\n",
    "\n",
    "            if len(partes) >= 2:\n",
    "                fonte = '_'.join(partes[1:-1])  # Tudo entre DICT_ e timestamp\n",
    "            else:\n",
    "                fonte = nome_base\n",
    "\n",
    "            # Carregar Excel\n",
    "            df_dict = pd.read_excel(arquivo)\n",
    "\n",
    "            schema = {\n",
    "                'fonte': fonte,\n",
    "                'arquivo_origem': arquivo.name,\n",
    "                'timestamp': partes[-1] if len(partes) > 2 else 'desconhecido',\n",
    "                'campos': {},\n",
    "                'metadados': {}\n",
    "            }\n",
    "\n",
    "            # Processar linhas do dicionário\n",
    "            for idx, row in df_dict.iterrows():\n",
    "                nome_campo = row.get('Campo', row.get('Coluna', f'campo_{idx}'))\n",
    "\n",
    "                schema['campos'][nome_campo] = {\n",
    "                    'tipo': row.get('Tipo', 'desconhecido'),\n",
    "                    'exemplo': row.get('Exemplo', ''),\n",
    "                    'descricao': row.get('Descrição', row.get('Descricao', '')),\n",
    "                    'posicao': idx\n",
    "                }\n",
    "\n",
    "            self.dicionarios_carregados[fonte] = schema\n",
    "            self.schemas_conhecidos[fonte] = list(schema['campos'].keys())\n",
    "\n",
    "            print(f\"   ✅ Carregado: {fonte} ({len(schema['campos'])} campos)\")\n",
    "\n",
    "            return schema\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Erro ao carregar {arquivo.name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def carregar_todos_dicionarios(self) -> None:\n",
    "        \"\"\"Carrega todos os dicionários existentes.\"\"\"\n",
    "\n",
    "        print(\"\\n📚 Carregando Dicionários Persistentes\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        dicionarios = self.buscar_dicionarios_existentes()\n",
    "\n",
    "        if not dicionarios:\n",
    "            print(\"   ℹ️  Nenhum dicionário encontrado\")\n",
    "            print(\"   → Será criado automaticamente ao processar primeiro arquivo\")\n",
    "            return\n",
    "\n",
    "        print()\n",
    "        for dict_file in dicionarios:\n",
    "            self.carregar_dicionario(dict_file)\n",
    "\n",
    "        print()\n",
    "        print(f\"✅ Total carregado: {len(self.dicionarios_carregados)} dicionário(s)\")\n",
    "\n",
    "    def detectar_fonte_arquivo(self, nome_arquivo: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Detecta fonte do arquivo pelo nome.\n",
    "\n",
    "        Args:\n",
    "            nome_arquivo: Nome do arquivo\n",
    "\n",
    "        Returns:\n",
    "            Nome da fonte ou None\n",
    "        \"\"\"\n",
    "        nome_lower = nome_arquivo.lower()\n",
    "\n",
    "        # Padrões conhecidos\n",
    "        padroes = {\n",
    "            'YSMM_VI_ACOMP': ['ysmm', 'vi_acomp', 'variacao', 'interna'],\n",
    "            'PORTAL_ESO': ['eso', 'solicitac', 'pedido', 'revisao'],\n",
    "            'BEX_ESTOQUE': ['bex', 'estoque', 'movimentacao'],\n",
    "            'GRUPOS_PRODUTOS': ['grupo', 'produto', 'material'],\n",
    "            'CENTROS_BR': ['centro', 'base', 'sigla']\n",
    "        }\n",
    "\n",
    "        for fonte, keywords in padroes.items():\n",
    "            if any(kw in nome_lower for kw in keywords):\n",
    "                return fonte\n",
    "\n",
    "        return None\n",
    "\n",
    "    def mapear_campos_automaticamente(self, df: pd.DataFrame, fonte: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Mapeia campos do DataFrame usando dicionário da fonte.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame a mapear\n",
    "            fonte: Nome da fonte\n",
    "\n",
    "        Returns:\n",
    "            Dict com mapeamento campo_original → campo_padrao\n",
    "        \"\"\"\n",
    "        mapeamento = {}\n",
    "\n",
    "        if fonte not in self.schemas_conhecidos:\n",
    "            print(f\"   ⚠️  Fonte '{fonte}' não encontrada nos dicionários\")\n",
    "            return mapeamento\n",
    "\n",
    "        campos_conhecidos = self.schemas_conhecidos[fonte]\n",
    "\n",
    "        # Mapear por similaridade\n",
    "        for col_original in df.columns:\n",
    "            melhor_match = None\n",
    "            melhor_score = 0.0\n",
    "\n",
    "            for campo_conhecido in campos_conhecidos:\n",
    "                score = calcular_similaridade(col_original, campo_conhecido)\n",
    "\n",
    "                if score > melhor_score:\n",
    "                    melhor_score = score\n",
    "                    melhor_match = campo_conhecido\n",
    "\n",
    "            # Threshold de 70%\n",
    "            if melhor_score >= 0.70:\n",
    "                mapeamento[col_original] = {\n",
    "                    'campo_padrao': melhor_match,\n",
    "                    'confianca': melhor_score\n",
    "                }\n",
    "\n",
    "        return mapeamento\n",
    "\n",
    "    def processar_arquivo_novo(self, arquivo: Path, linha_cabecalho: Optional[int] = None,\n",
    "                              linha_dados_inicio: Optional[int] = None) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \"\"\"\n",
    "        Processa arquivo novo com detecção automática ou parâmetros manuais.\n",
    "\n",
    "        Args:\n",
    "            arquivo: Path do arquivo a processar\n",
    "            linha_cabecalho: Linha do cabeçalho (None = auto-detectar)\n",
    "            linha_dados_inicio: Linha de início dos dados (None = logo após cabeçalho)\n",
    "\n",
    "        Returns:\n",
    "            Tuple (DataFrame carregado, Dict com metadados da detecção)\n",
    "        \"\"\"\n",
    "        print(f\"\\n🔍 Processando Arquivo Novo: {arquivo.name}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        # Detectar fonte\n",
    "        fonte_detectada = self.detectar_fonte_arquivo(arquivo.name)\n",
    "        print(f\"   Fonte detectada: {fonte_detectada if fonte_detectada else 'DESCONHECIDA'}\")\n",
    "\n",
    "        # Carregar arquivo bruto\n",
    "        if arquivo.suffix == '.xlsx':\n",
    "            df_bruto = pd.read_excel(arquivo, header=None)\n",
    "        elif arquivo.suffix == '.xls':\n",
    "            df_bruto = pd.read_excel(arquivo, header=None)\n",
    "        elif arquivo.suffix == '.csv':\n",
    "            df_bruto = pd.read_csv(arquivo, header=None)\n",
    "        else:\n",
    "            raise ValueError(f\"Formato não suportado: {arquivo.suffix}\")\n",
    "\n",
    "        print(f\"   Arquivo bruto: {len(df_bruto)} linhas × {len(df_bruto.columns)} colunas\")\n",
    "\n",
    "        # AUTO-DETECÇÃO de cabeçalho (se não fornecido)\n",
    "        if linha_cabecalho is None:\n",
    "            print(\"\\n   🔍 Auto-detectando cabeçalho...\")\n",
    "\n",
    "            # Termos que aparecem em cabeçalhos\n",
    "            termos_cabecalho = list(DICIONARIO_SINONIMOS.keys())\n",
    "\n",
    "            linha_cabecalho = identificar_linha_cabecalho(df_bruto, termos_cabecalho)\n",
    "\n",
    "            if linha_cabecalho >= 0:\n",
    "                print(f\"   ✅ Cabeçalho detectado na linha {linha_cabecalho} (Python) / linha {linha_cabecalho + 1} (Excel)\")\n",
    "            else:\n",
    "                print(\"   ⚠️  Cabeçalho não detectado automaticamente\")\n",
    "                print(\"   → Usando linha 0 como padrão\")\n",
    "                linha_cabecalho = 0\n",
    "        else:\n",
    "            print(f\"\\n   📍 Cabeçalho manual: linha {linha_cabecalho}\")\n",
    "\n",
    "        # Determinar linha de início dos dados\n",
    "        if linha_dados_inicio is None:\n",
    "            linha_dados_inicio = linha_cabecalho + 1\n",
    "\n",
    "        # Recarregar com cabeçalho correto\n",
    "        if arquivo.suffix in ['.xlsx', '.xls']:\n",
    "            df_carregado = pd.read_excel(arquivo, header=linha_cabecalho,\n",
    "                                        skiprows=list(range(linha_dados_inicio - linha_cabecalho - 1)) if linha_dados_inicio > linha_cabecalho + 1 else None)\n",
    "        else:\n",
    "            df_carregado = pd.read_csv(arquivo, header=linha_cabecalho,\n",
    "                                      skiprows=list(range(linha_dados_inicio - linha_cabecalho - 1)) if linha_dados_inicio > linha_cabecalho + 1 else None)\n",
    "\n",
    "        print(f\"\\n   ✅ Dados carregados: {len(df_carregado)} registros × {len(df_carregado.columns)} colunas\")\n",
    "\n",
    "        # Mapear campos (se fonte conhecida)\n",
    "        mapeamento = {}\n",
    "        if fonte_detectada and fonte_detectada in self.schemas_conhecidos:\n",
    "            print(f\"\\n   🔗 Mapeando campos usando dicionário de '{fonte_detectada}'...\")\n",
    "            mapeamento = self.mapear_campos_automaticamente(df_carregado, fonte_detectada)\n",
    "            print(f\"   ✅ {len(mapeamento)} campos mapeados\")\n",
    "\n",
    "        # Metadados da detecção\n",
    "        metadados = {\n",
    "            'arquivo': arquivo.name,\n",
    "            'fonte_detectada': fonte_detectada,\n",
    "            'linha_cabecalho': linha_cabecalho,\n",
    "            'linha_dados_inicio': linha_dados_inicio,\n",
    "            'total_linhas': len(df_carregado),\n",
    "            'total_colunas': len(df_carregado.columns),\n",
    "            'colunas': df_carregado.columns.tolist(),\n",
    "            'mapeamento': mapeamento,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        # Registrar no histórico\n",
    "        self.historico_deteccoes.append(metadados)\n",
    "\n",
    "        # Salvar metadados\n",
    "        self._salvar_metadados_deteccao(metadados, arquivo)\n",
    "\n",
    "        # Gerar código de reprodução\n",
    "        self._gerar_codigo_reproducao(metadados, arquivo)\n",
    "\n",
    "        return df_carregado, metadados\n",
    "\n",
    "    def _salvar_metadados_deteccao(self, metadados: Dict, arquivo: Path) -> None:\n",
    "        \"\"\"Salva metadados da detecção em JSON.\"\"\"\n",
    "\n",
    "        nome_base = arquivo.stem\n",
    "        arquivo_meta = self.pasta_dicionarios / f'META_{nome_base}_{self.fm.timestamp}.json'\n",
    "\n",
    "        with open(arquivo_meta, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadados, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"\\n   💾 Metadados salvos: {arquivo_meta.name}\")\n",
    "\n",
    "    def _gerar_codigo_reproducao(self, metadados: Dict, arquivo: Path) -> None:\n",
    "        \"\"\"Gera código Python para reproduzir o carregamento.\"\"\"\n",
    "\n",
    "        nome_base = arquivo.stem\n",
    "        arquivo_codigo = self.pasta_dicionarios / f'CODIGO_Reproducao_{nome_base}_{self.fm.timestamp}.py'\n",
    "\n",
    "        codigo = f'''# ═══════════════════════════════════════════════════════════════════\n",
    "# CÓDIGO PARA REPRODUZIR CARREGAMENTO\n",
    "# Gerado automaticamente pelo Sistema ETL-Integração\n",
    "# Data: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurações detectadas\n",
    "arquivo = Path(r\"{arquivo}\")\n",
    "linha_cabecalho = {metadados['linha_cabecalho']}  # Índice Python (linha Excel {metadados['linha_cabecalho'] + 1})\n",
    "\n",
    "# Carregar dados\n",
    "'''\n",
    "\n",
    "        if arquivo.suffix == '.xlsx':\n",
    "            codigo += f'''df = pd.read_excel(\n",
    "    arquivo,\n",
    "    header=linha_cabecalho\n",
    ")\n",
    "'''\n",
    "        elif arquivo.suffix == '.csv':\n",
    "            codigo += f'''df = pd.read_csv(\n",
    "    arquivo,\n",
    "    header=linha_cabecalho\n",
    ")\n",
    "'''\n",
    "\n",
    "        codigo += f'''\n",
    "print(f\"✅ Carregado: {{len(df):,}} registros × {{len(df.columns)}} colunas\")\n",
    "\n",
    "# Colunas detectadas\n",
    "colunas_esperadas = {metadados['colunas']}\n",
    "\n",
    "# Validar estrutura\n",
    "if df.columns.tolist() == colunas_esperadas:\n",
    "    print(\"✅ Estrutura validada!\")\n",
    "else:\n",
    "    print(\"⚠️  Estrutura diferente do esperado\")\n",
    "    print(f\"   Esperado: {{len(colunas_esperadas)}} colunas\")\n",
    "    print(f\"   Obtido: {{len(df.columns)}} colunas\")\n",
    "'''\n",
    "\n",
    "        with open(arquivo_codigo, 'w', encoding='utf-8') as f:\n",
    "            f.write(codigo)\n",
    "\n",
    "        print(f\"   💾 Código de reprodução: {arquivo_codigo.name}\")\n",
    "\n",
    "    def criar_dicionario_novo(self, df: pd.DataFrame, nome_fonte: str,\n",
    "                             arquivo_origem: str) -> Path:\n",
    "        \"\"\"\n",
    "        Cria novo dicionário persistente para uma fonte.\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame de referência\n",
    "            nome_fonte: Nome da fonte\n",
    "            arquivo_origem: Nome do arquivo original\n",
    "\n",
    "        Returns:\n",
    "            Path do dicionário criado\n",
    "        \"\"\"\n",
    "        print(f\"\\n📝 Criando Novo Dicionário: {nome_fonte}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        # Analisar campos\n",
    "        dados_dict = []\n",
    "\n",
    "        for i, col in enumerate(df.columns, 1):\n",
    "            serie = df[col]\n",
    "\n",
    "            # Análise básica\n",
    "            analise = analisar_coluna_completa(serie, amostra=100)\n",
    "\n",
    "            dados_dict.append({\n",
    "                'Posição': i,\n",
    "                'Campo': col,\n",
    "                'Tipo': str(serie.dtype),\n",
    "                'Tipo_Conteudo': analise['tipo_predominante'],\n",
    "                'Confiança_Detecção': f\"{analise['confianca']:.0%}\",\n",
    "                'Total_Valores': len(serie),\n",
    "                'Nulos': serie.isnull().sum(),\n",
    "                'Pct_Nulos': f\"{serie.isnull().sum() / len(serie) * 100:.1f}%\",\n",
    "                'Únicos': serie.nunique(),\n",
    "                'Exemplo_1': analise['exemplos'][0] if analise['exemplos'] else '',\n",
    "                'Exemplo_2': analise['exemplos'][1] if len(analise['exemplos']) > 1 else '',\n",
    "                'Exemplo_3': analise['exemplos'][2] if len(analise['exemplos']) > 2 else ''\n",
    "            })\n",
    "\n",
    "        df_dict = pd.DataFrame(dados_dict)\n",
    "\n",
    "        # Salvar\n",
    "        nome_arquivo = f'DICT_{nome_fonte}_{self.fm.timestamp}.xlsx'\n",
    "        caminho_dict = self.pasta_dicionarios / nome_arquivo\n",
    "\n",
    "        df_dict.to_excel(caminho_dict, index=False)\n",
    "\n",
    "        print(f\"   ✅ Dicionário criado: {nome_arquivo}\")\n",
    "        print(f\"   📊 {len(df_dict)} campos registrados\")\n",
    "\n",
    "        # Registrar no schema\n",
    "        schema = {\n",
    "            'fonte': nome_fonte,\n",
    "            'arquivo_origem': arquivo_origem,\n",
    "            'timestamp': self.fm.timestamp,\n",
    "            'campos': {row['Campo']: row.to_dict() for _, row in df_dict.iterrows()},\n",
    "            'metadados': {\n",
    "                'total_campos': len(df_dict),\n",
    "                'data_criacao': datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.dicionarios_carregados[nome_fonte] = schema\n",
    "        self.schemas_conhecidos[nome_fonte] = df_dict['Campo'].tolist()\n",
    "\n",
    "        # Registrar no FileManager\n",
    "        self.fm.logger.info(f\"Dicionário criado: {nome_fonte} ({len(df_dict)} campos)\")\n",
    "\n",
    "        return caminho_dict\n",
    "\n",
    "print(\"   ✅ Classe DicionarioDadosPersistente criada\")\n",
    "print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 2.2: INICIALIZAR DICIONÁRIO PERSISTENTE\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🚀 SEÇÃO 2.2: Inicializando Sistema de Dicionários\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Criar instância integrada com FileManager\n",
    "dicionario_persistente = DicionarioDadosPersistente(fm)\n",
    "\n",
    "# Carregar dicionários existentes\n",
    "dicionario_persistente.carregar_todos_dicionarios()\n",
    "\n",
    "# Criar alias para compatibilidade\n",
    "dict_pers = dicionario_persistente\n",
    "dp = dicionario_persistente\n",
    "\n",
    "print()\n",
    "print(\"✅ Sistema de Dicionários Persistentes ativo\")\n",
    "print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 2.3: FUNÇÃO AUXILIAR - PROCESSAR ARQUIVO COM GUI\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🎨 SEÇÃO 2.3: Função de Processamento com GUI\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def processar_arquivo_com_dialogo() -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Processa arquivo novo com diálogo interativo.\n",
    "\n",
    "    Permite usuário:\n",
    "    - Selecionar arquivo via GUI\n",
    "    - Especificar linha de cabeçalho (ou auto-detectar)\n",
    "    - Ver preview antes de confirmar\n",
    "\n",
    "    Returns:\n",
    "        Tuple (DataFrame processado, Metadados da detecção)\n",
    "    \"\"\"\n",
    "    print(\"\\n📂 Selecionando Arquivo...\")\n",
    "\n",
    "    # GUI para selecionar arquivo\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    arquivo = filedialog.askopenfilename(\n",
    "        title=\"Selecione Arquivo para Processar\",\n",
    "        filetypes=[\n",
    "            (\"Excel\", \"*.xlsx\"),\n",
    "            (\"Excel Antigo\", \"*.xls\"),\n",
    "            (\"CSV\", \"*.csv\"),\n",
    "            (\"Todos\", \"*.*\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if not arquivo:\n",
    "        raise ValueError(\"❌ Nenhum arquivo selecionado\")\n",
    "\n",
    "    arquivo_path = Path(arquivo)\n",
    "    print(f\"   ✅ Arquivo: {arquivo_path.name}\")\n",
    "\n",
    "    # Perguntar sobre linha de cabeçalho\n",
    "    print(\"\\n❓ Como detectar cabeçalho?\")\n",
    "    print(\"   1. Auto-detectar (recomendado)\")\n",
    "    print(\"   2. Especificar linha manualmente\")\n",
    "\n",
    "    escolha = input(\"   Escolha (1/2): \").strip()\n",
    "\n",
    "    linha_cabecalho = None\n",
    "    if escolha == '2':\n",
    "        linha_excel = input(\"   Linha do cabeçalho (Excel, ex: 4): \").strip()\n",
    "        try:\n",
    "            linha_cabecalho = int(linha_excel) - 1  # Converter para índice Python\n",
    "            print(f\"   ✅ Cabeçalho: linha {linha_cabecalho} (Python) / linha {linha_cabecalho + 1} (Excel)\")\n",
    "        except:\n",
    "            print(\"   ⚠️  Valor inválido, usando auto-detecção\")\n",
    "            linha_cabecalho = None\n",
    "\n",
    "    # Processar arquivo\n",
    "    df, metadados = dicionario_persistente.processar_arquivo_novo(\n",
    "        arquivo_path,\n",
    "        linha_cabecalho=linha_cabecalho\n",
    "    )\n",
    "\n",
    "    return df, metadados\n",
    "\n",
    "print(\"   ✅ processar_arquivo_com_dialogo()\")\n",
    "print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# RESULTADO FINAL DO BLOCO 2\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"╔\" + \"═\" * 78 + \"╗\")\n",
    "print(\"║\" + \" ✅ BLOCO 2 CONCLUÍDO COM SUCESSO \".center(78) + \"║\")\n",
    "print(\"╚\" + \"═\" * 78 + \"╝\")\n",
    "print()\n",
    "\n",
    "# Resumo\n",
    "formatar_tabela_resumo({\n",
    "    'sistema_dicionarios': 'ATIVO',\n",
    "    'dicionarios_carregados': len(dicionario_persistente.dicionarios_carregados),\n",
    "    'schemas_conhecidos': len(dicionario_persistente.schemas_conhecidos),\n",
    "    'pasta_dicionarios': dicionario_persistente.pasta_dicionarios.name,\n",
    "    'auto_deteccao': 'ATIVA',\n",
    "    'geracao_codigo': 'ATIVA',\n",
    "    'integracao_filemanager': 'COMPLETA'\n",
    "}, \"RESUMO DO BLOCO 2\")\n",
    "\n",
    "print()\n",
    "print(\"🔗 VARIÁVEIS GLOBAIS ADICIONADAS:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   • dicionario_persistente / dict_pers / dp (DicionarioDadosPersistente)\")\n",
    "print(\"   • processar_arquivo_com_dialogo() (função)\")\n",
    "\n",
    "print()\n",
    "print(\"📋 PRÓXIMOS PASSOS:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   1. ✅ Verifique se pasta 'dicionarios_persistentes' foi criada\")\n",
    "print(\"   2. ✅ Confirme quantos dicionários foram carregados\")\n",
    "print(\"   3. 📤 Envie: 'BLOCO 2 OK' para prosseguir\")\n",
    "print(\"   4. ⏭️  BLOCO 3 carregará arquivos usando este sistema\")\n",
    "\n",
    "print()\n",
    "print(\"🔗 CAMINHO DOS DICIONÁRIOS:\")\n",
    "print(f\"   {dicionario_persistente.pasta_dicionarios}\")\n",
    "print()\n",
    "print(\"═\" * 80)"
   ],
   "id": "8bb2d4c0c335de7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔══════════════════════════════════════════════════════════════════════════════╗\n",
      "║                  📚 BLOCO 2: DICIONÁRIO DE DADOS PERSISTENTE                  ║\n",
      "╚══════════════════════════════════════════════════════════════════════════════╝\n",
      "\n",
      "📖 SEÇÃO 2.1: Classe Dicionário Persistente\n",
      "--------------------------------------------------------------------------------\n",
      "   ✅ Classe DicionarioDadosPersistente criada\n",
      "\n",
      "🚀 SEÇÃO 2.2: Inicializando Sistema de Dicionários\n",
      "--------------------------------------------------------------------------------\n",
      "   ✅ Pasta de dicionários: dicionarios_persistentes\n",
      "\n",
      "\n",
      "📚 Carregando Dicionários Persistentes\n",
      "--------------------------------------------------------------------------------\n",
      "🔍 Buscando dicionários existentes...\n",
      "   Encontrados: 0 arquivo(s)\n",
      "   ℹ️  Nenhum dicionário encontrado\n",
      "   → Será criado automaticamente ao processar primeiro arquivo\n",
      "\n",
      "✅ Sistema de Dicionários Persistentes ativo\n",
      "\n",
      "🎨 SEÇÃO 2.3: Função de Processamento com GUI\n",
      "--------------------------------------------------------------------------------\n",
      "   ✅ processar_arquivo_com_dialogo()\n",
      "\n",
      "╔══════════════════════════════════════════════════════════════════════════════╗\n",
      "║                       ✅ BLOCO 2 CONCLUÍDO COM SUCESSO                        ║\n",
      "╚══════════════════════════════════════════════════════════════════════════════╝\n",
      "\n",
      "\n",
      "╔══════════════════════════════════════════════════════════════════════════════╗\n",
      "║                              RESUMO DO BLOCO 2                               ║\n",
      "╠══════════════════════════════════════════════════════════════════════════════╣\n",
      "║ Sistema Dicionarios                 :                                    ATIVO ║\n",
      "║ Dicionarios Carregados              :                                        0 ║\n",
      "║ Schemas Conhecidos                  :                                        0 ║\n",
      "║ Pasta Dicionarios                   :                 dicionarios_persistentes ║\n",
      "║ Auto Deteccao                       :                                    ATIVA ║\n",
      "║ Geracao Codigo                      :                                    ATIVA ║\n",
      "║ Integracao Filemanager              :                                 COMPLETA ║\n",
      "╚══════════════════════════════════════════════════════════════════════════════╝\n",
      "\n",
      "🔗 VARIÁVEIS GLOBAIS ADICIONADAS:\n",
      "--------------------------------------------------------------------------------\n",
      "   • dicionario_persistente / dict_pers / dp (DicionarioDadosPersistente)\n",
      "   • processar_arquivo_com_dialogo() (função)\n",
      "\n",
      "📋 PRÓXIMOS PASSOS:\n",
      "--------------------------------------------------------------------------------\n",
      "   1. ✅ Verifique se pasta 'dicionarios_persistentes' foi criada\n",
      "   2. ✅ Confirme quantos dicionários foram carregados\n",
      "   3. 📤 Envie: 'BLOCO 2 OK' para prosseguir\n",
      "   4. ⏭️  BLOCO 3 carregará arquivos usando este sistema\n",
      "\n",
      "🔗 CAMINHO DOS DICIONÁRIOS:\n",
      "   E:\\OneDrive - VIBRA\\NMCV - Documentos\\Indicador\\AIVI\\AIVI-INTEGRAÇÃO\\dicionarios_persistentes\n",
      "\n",
      "════════════════════════════════════════════════════════════════════════════════\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T14:51:26.044528Z",
     "start_time": "2025-10-17T14:51:14.868922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║              BLOCO 3: CARREGADOR MODULAR - ARQUIVO SAP (1/2)                 ║\n",
    "║                    YSMM_VI_ACOMP - Dados Históricos VI                       ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║  OBJETIVO:                                                                   ║\n",
    "║    • Carregar arquivo SAP BRUTO (YSMM_VI_ACOMP)                              ║\n",
    "║    • Aplicar limpeza automática (remover linhas/colunas metadados SAP)      ║\n",
    "║    • Detectar cabeçalho automaticamente                                      ║\n",
    "║    • Registrar no Dicionário Persistente                                     ║\n",
    "║    • Validar estrutura e qualidade                                           ║\n",
    "║    • Salvar dados limpos para análise                                        ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║  INTEGRAÇÃO:                                                                 ║\n",
    "║    • FileManager (BLOCO 1)                                                   ║\n",
    "║    • Dicionário Persistente (BLOCO 2)                                        ║\n",
    "║    • Funções auxiliares (BLOCO 1 - regex, validações)                        ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 3.1: GUI PARA SELEÇÃO DO ARQUIVO SAP\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"╔\" + \"═\" * 78 + \"╗\")\n",
    "print(\"║\" + \" 📂 BLOCO 3: CARREGADOR SAP (YSMM_VI_ACOMP)\".center(78) + \"║\")\n",
    "print(\"╚\" + \"═\" * 78 + \"╝\")\n",
    "print()\n",
    "\n",
    "print(\"📂 SEÇÃO 3.1: Seleção do Arquivo SAP\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "print(\"🎯 ARQUIVO A SELECIONAR:\")\n",
    "print(\"   Nome: YSMM_VI_ACOMP ou similar (exemplo: 2025YSMMVIMONITORRECALC)\")\n",
    "print(\"   Fonte: SAP ECC 6.0 - Transação YSMM_VI_ACOMP\")\n",
    "print(\"   Formato: .xlsx ou .xls (ORIGINAL, sem limpeza prévia)\")\n",
    "print(\"   Conteúdo: Dados históricos de Variação Interna por Base/Produto\")\n",
    "print()\n",
    "print(\"⚠️  IMPORTANTE:\")\n",
    "print(\"   • Selecione o arquivo BRUTO (como exportado do SAP)\")\n",
    "print(\"   • NÃO use arquivo já processado/limpo manualmente\")\n",
    "print(\"   • O sistema fará a limpeza automática\")\n",
    "print()\n",
    "\n",
    "# Criar janela de seleção GRANDE e CLARA\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "# Trazer para frente\n",
    "root.attributes('-topmost', True)\n",
    "root.update()\n",
    "\n",
    "arquivo_sap_path = filedialog.askopenfilename(\n",
    "    title=\"📂 SELECIONE: Arquivo SAP YSMM_VI_ACOMP (BRUTO)\",\n",
    "    filetypes=[\n",
    "        (\"Excel Novo\", \"*.xlsx\"),\n",
    "        (\"Excel Antigo\", \"*.xls\"),\n",
    "        (\"Todos Excel\", \"*.xls*\"),\n",
    "        (\"Todos\", \"*.*\")\n",
    "    ],\n",
    "    initialdir=fm.diretorio_base\n",
    ")\n",
    "\n",
    "root.destroy()\n",
    "\n",
    "if not arquivo_sap_path:\n",
    "    raise ValueError(\"❌ Nenhum arquivo selecionado. Execute novamente o bloco.\")\n",
    "\n",
    "arquivo_sap = Path(arquivo_sap_path)\n",
    "\n",
    "print(f\"✅ Arquivo selecionado: {arquivo_sap.name}\")\n",
    "print(f\"   Caminho: {arquivo_sap.parent}\")\n",
    "print(f\"   Tamanho: {arquivo_sap.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "print()\n",
    "\n",
    "# Registrar no FileManager\n",
    "fm.logger.info(f\"Arquivo SAP selecionado: {arquivo_sap.name}\")\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 3.2: CARREGAMENTO BRUTO E ANÁLISE INICIAL\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"📊 SEÇÃO 3.2: Carregamento e Análise Inicial\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Carregar TUDO sem processar (para inspeção)\n",
    "print(\"\\n🔍 Carregando arquivo bruto...\")\n",
    "\n",
    "if arquivo_sap.suffix == '.xlsx':\n",
    "    # Tentar detectar sheet\n",
    "    xls = pd.ExcelFile(arquivo_sap)\n",
    "    sheets = xls.sheet_names\n",
    "\n",
    "    print(f\"   Sheets encontradas: {len(sheets)}\")\n",
    "    for i, sheet in enumerate(sheets, 1):\n",
    "        print(f\"      {i}. {sheet}\")\n",
    "\n",
    "    # Usar primeira sheet (ou detectar por padrão)\n",
    "    sheet_usar = sheets[0]\n",
    "    for sheet in sheets:\n",
    "        if any(termo in sheet.lower() for termo in ['ysmm', 'vi', 'monitor', 'acomp']):\n",
    "            sheet_usar = sheet\n",
    "            break\n",
    "\n",
    "    print(f\"   → Usando sheet: '{sheet_usar}'\")\n",
    "\n",
    "    df_bruto = pd.read_excel(arquivo_sap, sheet_name=sheet_usar, header=None)\n",
    "\n",
    "else:  # .xls\n",
    "    df_bruto = pd.read_excel(arquivo_sap, header=None)\n",
    "    sheet_usar = 'Sheet1'\n",
    "\n",
    "print(f\"\\n✅ Arquivo carregado (bruto)\")\n",
    "print(f\"   Dimensões: {len(df_bruto):,} linhas × {len(df_bruto.columns)} colunas\")\n",
    "print()\n",
    "\n",
    "# Preview das primeiras linhas (metadados SAP típicos)\n",
    "print(\"📋 Preview das primeiras 10 linhas (formato bruto):\")\n",
    "print(\"-\" * 80)\n",
    "for i in range(min(10, len(df_bruto))):\n",
    "    valores = df_bruto.iloc[i, :5].tolist()  # Primeiras 5 colunas\n",
    "    valores_str = ' | '.join([str(v)[:20] if pd.notna(v) else 'NULL' for v in valores])\n",
    "    print(f\"   Linha {i:2d}: {valores_str}\")\n",
    "print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 3.3: LIMPEZA AUTOMÁTICA - REMOÇÃO DE METADADOS SAP\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🧹 SEÇÃO 3.3: Limpeza Automática\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "\n",
    "# Identificar colunas vazias ou de metadados (coluna A geralmente)\n",
    "colunas_vazias = []\n",
    "for col in df_bruto.columns:\n",
    "    pct_vazios = df_bruto[col].isnull().sum() / len(df_bruto)\n",
    "    if pct_vazios > 0.95:  # >95% vazio\n",
    "        colunas_vazias.append(col)\n",
    "\n",
    "if colunas_vazias:\n",
    "    print(f\"🗑️  Colunas com >95% vazios detectadas: {colunas_vazias}\")\n",
    "    print(f\"   → Serão removidas\")\n",
    "else:\n",
    "    print(\"✅ Nenhuma coluna vazia detectada\")\n",
    "\n",
    "# Identificar linhas de metadados (típico em SAP: linhas iniciais com títulos, filtros, etc)\n",
    "print(\"\\n🔍 Identificando linhas de metadados SAP...\")\n",
    "\n",
    "linhas_remover = []\n",
    "\n",
    "# Detectar linhas que são claramente metadados\n",
    "for idx in range(min(10, len(df_bruto))):\n",
    "    row = df_bruto.iloc[idx]\n",
    "\n",
    "    # Critérios para metadados:\n",
    "    # 1. Menos de 30% das células preenchidas\n",
    "    # 2. Contém palavras-chave de metadados SAP\n",
    "    pct_preenchido = row.notna().sum() / len(row)\n",
    "\n",
    "    valores_str = ' '.join([str(v).lower() for v in row if pd.notna(v)])\n",
    "\n",
    "    keywords_metadados = ['sap', 'relatório', 'relatorio', 'query', 'consulta',\n",
    "                          'data:', 'hora:', 'usuário', 'usuario', 'página', 'pagina']\n",
    "\n",
    "    tem_keyword = any(kw in valores_str for kw in keywords_metadados)\n",
    "\n",
    "    if pct_preenchido < 0.3 or tem_keyword:\n",
    "        linhas_remover.append(idx)\n",
    "        print(f\"   Linha {idx}: METADADO detectado (remover)\")\n",
    "\n",
    "print()\n",
    "if linhas_remover:\n",
    "    print(f\"🗑️  {len(linhas_remover)} linhas de metadados serão removidas\")\n",
    "else:\n",
    "    print(\"✅ Nenhuma linha de metadado detectada\")\n",
    "\n",
    "# Aplicar remoção\n",
    "df_sem_metadados = df_bruto.copy()\n",
    "\n",
    "if colunas_vazias:\n",
    "    df_sem_metadados = df_sem_metadados.drop(columns=colunas_vazias)\n",
    "    print(f\"\\n✅ Colunas removidas: {len(colunas_vazias)}\")\n",
    "\n",
    "if linhas_remover:\n",
    "    df_sem_metadados = df_sem_metadados.drop(index=linhas_remover)\n",
    "    df_sem_metadados = df_sem_metadados.reset_index(drop=True)\n",
    "    print(f\"✅ Linhas removidas: {len(linhas_remover)}\")\n",
    "\n",
    "print()\n",
    "print(f\"📊 Após limpeza inicial:\")\n",
    "print(f\"   Dimensões: {len(df_sem_metadados):,} linhas × {len(df_sem_metadados.columns)} colunas\")\n",
    "print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 3.4: DETECÇÃO AUTOMÁTICA DE CABEÇALHO\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"🔍 SEÇÃO 3.4: Detecção de Cabeçalho\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "\n",
    "# Usar função do BLOCO 1 integrada com Dicionário Persistente\n",
    "termos_cabecalho = [\n",
    "    'Centro', 'Sigla', 'Base', 'Produto', 'Grupo',\n",
    "    'Expedição', 'Variação', 'Interna', 'VI',\n",
    "    'Limite', 'Mês', 'Ano', 'Período'\n",
    "]\n",
    "\n",
    "linha_cabecalho = identificar_linha_cabecalho(df_sem_metadados, termos_cabecalho)\n",
    "\n",
    "if linha_cabecalho < 0:\n",
    "    print(\"⚠️  Cabeçalho não detectado automaticamente\")\n",
    "    print(\"   → Tentando heurística alternativa...\")\n",
    "\n",
    "    # Heurística: linha com maior diversidade de tipos\n",
    "    for idx in range(min(5, len(df_sem_metadados))):\n",
    "        row = df_sem_metadados.iloc[idx]\n",
    "\n",
    "        # Checar se tem texto (não números)\n",
    "        tem_texto = row.apply(lambda x: isinstance(x, str)).sum() > len(row) * 0.5\n",
    "\n",
    "        if tem_texto:\n",
    "            linha_cabecalho = idx\n",
    "            print(f\"   ✅ Cabeçalho detectado por heurística: linha {linha_cabecalho}\")\n",
    "            break\n",
    "\n",
    "    if linha_cabecalho < 0:\n",
    "        linha_cabecalho = 0\n",
    "        print(\"   ⚠️  Usando linha 0 como padrão\")\n",
    "\n",
    "print()\n",
    "print(f\"✅ Cabeçalho identificado:\")\n",
    "print(f\"   Linha: {linha_cabecalho} (Python) / Linha {linha_cabecalho + 1} (Excel)\")\n",
    "print()\n",
    "\n",
    "# Preview do cabeçalho detectado\n",
    "print(\"📋 Colunas detectadas:\")\n",
    "cabecalho_preview = df_sem_metadados.iloc[linha_cabecalho].tolist()\n",
    "for i, col in enumerate(cabecalho_preview[:10], 1):  # Primeiras 10\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "if len(cabecalho_preview) > 10:\n",
    "    print(f\"   ... e mais {len(cabecalho_preview) - 10} colunas\")\n",
    "print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 3.5: RECARREGAR COM CABEÇALHO CORRETO\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"📥 SEÇÃO 3.5: Recarga com Estrutura Correta\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "\n",
    "# Recarregar usando linha de cabeçalho detectada\n",
    "if arquivo_sap.suffix == '.xlsx':\n",
    "    df_sap_limpo = pd.read_excel(\n",
    "        arquivo_sap,\n",
    "        sheet_name=sheet_usar,\n",
    "        header=linha_cabecalho + len(linhas_remover),  # Ajustar offset das linhas removidas\n",
    "        skiprows=[i for i in linhas_remover if i < linha_cabecalho]\n",
    "    )\n",
    "else:\n",
    "    df_sap_limpo = pd.read_excel(\n",
    "        arquivo_sap,\n",
    "        header=linha_cabecalho + len(linhas_remover),\n",
    "        skiprows=[i for i in linhas_remover if i < linha_cabecalho]\n",
    "    )\n",
    "\n",
    "# Remover colunas vazias (se ainda houver)\n",
    "if colunas_vazias:\n",
    "    colunas_manter = [c for c in df_sap_limpo.columns if c not in colunas_vazias]\n",
    "    df_sap_limpo = df_sap_limpo[colunas_manter]\n",
    "\n",
    "print(f\"✅ Dados recarregados com estrutura correta\")\n",
    "print(f\"   Registros: {len(df_sap_limpo):,}\")\n",
    "print(f\"   Colunas: {len(df_sap_limpo.columns)}\")\n",
    "print()\n",
    "\n",
    "# Preview dos dados limpos\n",
    "print(\"📋 Preview dos dados limpos (primeiras 5 linhas):\")\n",
    "print(\"-\" * 80)\n",
    "print(df_sap_limpo.head())\n",
    "print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 3.6: VALIDAÇÃO INLINE\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"✔️  SEÇÃO 3.6: Validação de Transformação\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Validar limpeza (bruto → limpo)\n",
    "validacao = validar_transformacao(\n",
    "    df_bruto,\n",
    "    df_sap_limpo,\n",
    "    'Limpeza SAP',\n",
    "    permitir_reducao=True\n",
    ")\n",
    "\n",
    "if not validacao['passou']:\n",
    "    print()\n",
    "    print(\"⚠️  ALERTAS DE VALIDAÇÃO:\")\n",
    "    for alerta in validacao['alertas']:\n",
    "        print(f\"   {alerta}\")\n",
    "    print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 3.7: REGISTRAR NO DICIONÁRIO PERSISTENTE\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print()\n",
    "print(\"📚 SEÇÃO 3.7: Registro no Dicionário Persistente\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Detectar fonte\n",
    "fonte_detectada = dicionario_persistente.detectar_fonte_arquivo(arquivo_sap.name)\n",
    "\n",
    "if not fonte_detectada:\n",
    "    fonte_detectada = 'YSMM_VI_ACOMP'\n",
    "    print(f\"   ℹ️  Fonte não detectada automaticamente\")\n",
    "    print(f\"   → Usando: {fonte_detectada}\")\n",
    "else:\n",
    "    print(f\"   ✅ Fonte detectada: {fonte_detectada}\")\n",
    "\n",
    "# Criar/atualizar dicionário\n",
    "if fonte_detectada not in dicionario_persistente.schemas_conhecidos:\n",
    "    print(f\"\\n📝 Criando novo dicionário para '{fonte_detectada}'...\")\n",
    "\n",
    "    dict_path = dicionario_persistente.criar_dicionario_novo(\n",
    "        df_sap_limpo,\n",
    "        fonte_detectada,\n",
    "        arquivo_sap.name\n",
    "    )\n",
    "\n",
    "    print(f\"   ✅ Dicionário criado: {dict_path.name}\")\n",
    "else:\n",
    "    print(f\"\\n✅ Dicionário já existe para '{fonte_detectada}'\")\n",
    "\n",
    "# Registrar schema no FileManager (PARTE 12.3)\n",
    "schema_log_path = fm.registrar_schema_campos(\n",
    "    df_sap_limpo,\n",
    "    fonte_detectada,\n",
    "    arquivo_sap.name\n",
    ")\n",
    "\n",
    "print(f\"   ✅ Schema registrado em logs: {schema_log_path.name}\")\n",
    "print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 3.8: SALVAR DADOS LIMPOS\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"💾 SEÇÃO 3.8: Salvamento dos Dados Limpos\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Salvar em dados_processados\n",
    "arquivo_limpo = fm.diretorios['dados_processados'] / f'SAP_YSMM_VI_ACOMP_LIMPO_{fm.timestamp}.csv'\n",
    "df_sap_limpo.to_csv(arquivo_limpo, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"✅ Dados limpos salvos:\")\n",
    "print(f\"   Arquivo: {arquivo_limpo.name}\")\n",
    "print(f\"   Tamanho: {arquivo_limpo.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "print()\n",
    "\n",
    "# Registrar no FileManager\n",
    "fm.logger.info(f\"Dados SAP limpos salvos: {arquivo_limpo.name}\")\n",
    "\n",
    "# Copiar arquivo original para dados_entrada\n",
    "arquivo_entrada_destino = fm.diretorios['dados_entrada'] / arquivo_sap.name\n",
    "if not arquivo_entrada_destino.exists():\n",
    "    import shutil\n",
    "    shutil.copy2(arquivo_sap, arquivo_entrada_destino)\n",
    "    print(f\"✅ Arquivo original copiado para: {arquivo_entrada_destino.relative_to(fm.diretorio_execucao)}\")\n",
    "    fm.logger.info(f\"Arquivo original copiado: {arquivo_sap.name}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# SEÇÃO 3.9: ANÁLISE ESTATÍSTICA RÁPIDA\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"📊 SEÇÃO 3.9: Análise Estatística Rápida\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "\n",
    "# Campos numéricos\n",
    "campos_numericos = df_sap_limpo.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"📈 Campos numéricos: {len(campos_numericos)}\")\n",
    "\n",
    "if campos_numericos:\n",
    "    print()\n",
    "    for campo in campos_numericos[:5]:  # Primeiros 5\n",
    "        serie = df_sap_limpo[campo]\n",
    "        print(f\"   {campo}:\")\n",
    "        print(f\"      Média: {serie.mean():,.2f}\")\n",
    "        print(f\"      Min: {serie.min():,.2f} | Max: {serie.max():,.2f}\")\n",
    "        print(f\"      NULLs: {serie.isnull().sum():,}\")\n",
    "        print()\n",
    "\n",
    "# Campos categóricos\n",
    "campos_categoricos = df_sap_limpo.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"📝 Campos categóricos: {len(campos_categoricos)}\")\n",
    "\n",
    "if campos_categoricos:\n",
    "    print()\n",
    "    for campo in campos_categoricos[:3]:  # Primeiros 3\n",
    "        n_unicos = df_sap_limpo[campo].nunique()\n",
    "        print(f\"   {campo}: {n_unicos:,} valores únicos\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# RESULTADO FINAL DO BLOCO 3\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"╔\" + \"═\" * 78 + \"╗\")\n",
    "print(\"║\" + \" ✅ BLOCO 3 CONCLUÍDO COM SUCESSO \".center(78) + \"║\")\n",
    "print(\"╚\" + \"═\" * 78 + \"╝\")\n",
    "print()\n",
    "\n",
    "# Resumo em tabela\n",
    "formatar_tabela_resumo({\n",
    "    'arquivo_original': arquivo_sap.name,\n",
    "    'fonte_detectada': fonte_detectada,\n",
    "    'linhas_bruto': len(df_bruto),\n",
    "    'linhas_limpo': len(df_sap_limpo),\n",
    "    'colunas_limpo': len(df_sap_limpo.columns),\n",
    "    'linha_cabecalho': f\"Linha {linha_cabecalho + 1} (Excel)\",\n",
    "    'linhas_metadados_removidas': len(linhas_remover),\n",
    "    'colunas_vazias_removidas': len(colunas_vazias),\n",
    "    'arquivo_salvo': arquivo_limpo.name,\n",
    "    'tamanho_mb': f\"{arquivo_limpo.stat().st_size / 1024 / 1024:.2f}\",\n",
    "    'dicionario_registrado': 'SIM',\n",
    "    'schema_logs': 'SIM'\n",
    "}, \"RESUMO DO BLOCO 3 - ARQUIVO SAP\")\n",
    "\n",
    "print()\n",
    "print(\"🔗 VARIÁVEL GLOBAL CRIADA:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   • df_sap_limpo (DataFrame com dados SAP processados)\")\n",
    "\n",
    "print()\n",
    "print(\"📋 PRÓXIMOS PASSOS:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"   1. ✅ Verifique os dados em df_sap_limpo.head()\")\n",
    "print(\"   2. ✅ Confirme arquivo salvo em dados_processados/\")\n",
    "print(\"   3. ✅ Verifique dicionário criado em dicionarios_persistentes/\")\n",
    "print(\"   4. 📤 Envie: 'BLOCO 3 OK' para prosseguir\")\n",
    "print(\"   5. ⏭️  BLOCO 4 carregará arquivo de Solicitações (Portal ESO)\")\n",
    "\n",
    "print()\n",
    "print(\"═\" * 80)"
   ],
   "id": "e6ebf0abda5c4eb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔══════════════════════════════════════════════════════════════════════════════╗\n",
      "║                   📂 BLOCO 3: CARREGADOR SAP (YSMM_VI_ACOMP)                  ║\n",
      "╚══════════════════════════════════════════════════════════════════════════════╝\n",
      "\n",
      "📂 SEÇÃO 3.1: Seleção do Arquivo SAP\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "🎯 ARQUIVO A SELECIONAR:\n",
      "   Nome: YSMM_VI_ACOMP ou similar (exemplo: 2025YSMMVIMONITORRECALC)\n",
      "   Fonte: SAP ECC 6.0 - Transação YSMM_VI_ACOMP\n",
      "   Formato: .xlsx ou .xls (ORIGINAL, sem limpeza prévia)\n",
      "   Conteúdo: Dados históricos de Variação Interna por Base/Produto\n",
      "\n",
      "⚠️  IMPORTANTE:\n",
      "   • Selecione o arquivo BRUTO (como exportado do SAP)\n",
      "   • NÃO use arquivo já processado/limpo manualmente\n",
      "   • O sistema fará a limpeza automática\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "❌ Nenhum arquivo selecionado. Execute novamente o bloco.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 67\u001B[39m\n\u001B[32m     64\u001B[39m root.destroy()\n\u001B[32m     66\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m arquivo_sap_path:\n\u001B[32m---> \u001B[39m\u001B[32m67\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33m❌ Nenhum arquivo selecionado. Execute novamente o bloco.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     69\u001B[39m arquivo_sap = Path(arquivo_sap_path)\n\u001B[32m     71\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m✅ Arquivo selecionado: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00marquivo_sap.name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mValueError\u001B[39m: ❌ Nenhum arquivo selecionado. Execute novamente o bloco."
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "AIVI DATA INTEGRATION - BLOCO 3: CARREGADOR MODULAR - ARQUIVO 2\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "Arquivo: ysmm_centros_br.xlsx\n",
    "Tipo: Tabela de Centros/Bases (Dados Complementares)\n",
    "\n",
    "LIMPEZA:\n",
    "  ✅ Primeira linha é o cabeçalho (usar header=0)\n",
    "  ✅ Sem colunas iniciais a expurgar\n",
    "\n",
    "FUNCIONALIDADES:\n",
    "  ✅ Carregamento robusto (.XLS/.XLSX)\n",
    "  ✅ Listagem completa de colunas\n",
    "  ✅ Preview dos dados\n",
    "  ✅ Backup automático\n",
    "  ✅ Logging\n",
    "═══════════════════════════════════════════════════════════════════════════════\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "\n",
    "print(\"╔\" + \"═\" * 78 + \"╗\")\n",
    "print(\"║\" + \" BLOCO 3: CARREGADOR MODULAR - ARQUIVO 2/N \".center(78) + \"║\")\n",
    "print(\"╚\" + \"═\" * 78 + \"╝\")\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# FUNÇÃO: Carregar ysmm_centros_br.xlsx\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def carregar_ysmm_centros():\n",
    "    \"\"\"\n",
    "    Carrega arquivo ysmm_centros_br.xlsx (tabela de centros/bases).\n",
    "\n",
    "    Características:\n",
    "      - Primeira linha = cabeçalho\n",
    "      - Sem limpeza de colunas necessária\n",
    "      - Dados complementares para enriquecer base SAP\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"╔\" + \"═\" * 78 + \"╗\")\n",
    "    print(\"║\" + \" ARQUIVO 2: ysmm_centros_br.xlsx (TABELA DE CENTROS) \".center(78) + \"║\")\n",
    "    print(\"╚\" + \"═\" * 78 + \"╝\")\n",
    "    print()\n",
    "\n",
    "    # ══════════════════════════════════════════════════════════════════════\n",
    "    # ETAPA 1: Seleção do arquivo\n",
    "    # ══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "    mensagem = \"\"\"\n",
    "╔═══════════════════════════════════════════════════╗\n",
    "║  📂 ARQUIVO: ysmm_centros_br.xlsx                 ║\n",
    "╠═══════════════════════════════════════════════════╣\n",
    "║                                                   ║\n",
    "║  Conteúdo: Cadastro de Centros/Bases              ║\n",
    "║  Uso: Enriquecimento dos dados SAP                ║\n",
    "║                                                   ║\n",
    "║  Campos esperados:                                ║\n",
    "║  • Centro (código)                                ║\n",
    "║  • Sigla                                          ║\n",
    "║  • Nome/Região                                    ║\n",
    "║  • Outros atributos da base                       ║\n",
    "║                                                   ║\n",
    "║  ⚠️  Primeira linha é o cabeçalho                 ║\n",
    "║                                                   ║\n",
    "╚═══════════════════════════════════════════════════╝\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Seleção GUI\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()\n",
    "        root.lift()\n",
    "        root.attributes('-topmost', True)\n",
    "        root.attributes('-alpha', 0.0)\n",
    "        root.update()\n",
    "\n",
    "        messagebox.showinfo(\n",
    "            \"[2/N] ysmm_centros_br.xlsx\",\n",
    "            mensagem.strip()\n",
    "        )\n",
    "\n",
    "        arquivo = filedialog.askopenfilename(\n",
    "            title=\"[2/N] Selecione ysmm_centros_br.xlsx\",\n",
    "            filetypes=[\n",
    "                (\"Excel files\", \"*.xlsx *.xls\"),\n",
    "                (\"All files\", \"*.*\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        root.destroy()\n",
    "\n",
    "        if not arquivo:\n",
    "            raise ValueError(\"❌ Nenhum arquivo selecionado\")\n",
    "\n",
    "        arquivo_path = Path(arquivo)\n",
    "\n",
    "        print(f\"✅ Arquivo selecionado:\")\n",
    "        print(f\"   📁 Nome: {arquivo_path.name}\")\n",
    "        print(f\"   📊 Tamanho: {arquivo_path.stat().st_size:,} bytes\")\n",
    "        print(f\"   🔧 Tipo: {arquivo_path.suffix}\")\n",
    "        print()\n",
    "\n",
    "        # ══════════════════════════════════════════════════════════════════\n",
    "        # ETAPA 2: Carregamento\n",
    "        # ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "        extensao = arquivo_path.suffix.lower()\n",
    "\n",
    "        print(f\"📖 Carregando arquivo Excel ({extensao})...\")\n",
    "\n",
    "        # Tentar engines\n",
    "        if extensao == '.xlsx':\n",
    "            engines = ['openpyxl', None]\n",
    "        elif extensao == '.xls':\n",
    "            engines = ['xlrd', None]\n",
    "        else:\n",
    "            engines = [None, 'openpyxl', 'xlrd']\n",
    "\n",
    "        df = None\n",
    "        for i, engine in enumerate(engines, 1):\n",
    "            try:\n",
    "                if engine:\n",
    "                    print(f\"   Tentativa {i}/{len(engines)}: engine='{engine}'\")\n",
    "                    df = pd.read_excel(arquivo_path, header=0, engine=engine)\n",
    "                else:\n",
    "                    print(f\"   Tentativa {i}/{len(engines)}: engine automático\")\n",
    "                    df = pd.read_excel(arquivo_path, header=0)\n",
    "\n",
    "                print(f\"   ✅ Sucesso! {df.shape[0]:,} linhas × {df.shape[1]} colunas\")\n",
    "                fm.logger.info(f\"Carregado: {arquivo_path.name} ({df.shape[0]} × {df.shape[1]})\")\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Falhou: {str(e)[:60]}...\")\n",
    "                if i == len(engines):\n",
    "                    if extensao == '.xls':\n",
    "                        print()\n",
    "                        print(\"💡 SOLUÇÃO para .XLS:\")\n",
    "                        print(\"   pip install xlrd\")\n",
    "                    raise Exception(f\"Falha ao ler: {e}\")\n",
    "\n",
    "        if df is None:\n",
    "            raise Exception(\"Nenhum engine funcionou\")\n",
    "\n",
    "        print()\n",
    "\n",
    "        # ══════════════════════════════════════════════════════════════════\n",
    "        # ETAPA 3: Limpeza (não necessária para este arquivo)\n",
    "        # ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "        print(\"🧹 Limpeza de dados...\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"   ✅ Nenhuma limpeza necessária (primeira linha = cabeçalho)\")\n",
    "        print(f\"   📊 Dados prontos: {df.shape[0]:,} linhas × {df.shape[1]} colunas\")\n",
    "        print()\n",
    "\n",
    "        # ══════════════════════════════════════════════════════════════════\n",
    "        # ETAPA 4: Listar todas as colunas\n",
    "        # ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "        print(\"📑 LISTAGEM COMPLETA DE COLUNAS\")\n",
    "        print(\"═\" * 80)\n",
    "        print(f\"📏 Total: {len(df.columns)} colunas\")\n",
    "        print()\n",
    "\n",
    "        for i, col in enumerate(df.columns, 1):\n",
    "            tipo = df[col].dtype\n",
    "            nulos = df[col].isna().sum()\n",
    "            pct_nulos = (nulos / len(df) * 100) if len(df) > 0 else 0\n",
    "            unicos = df[col].nunique()\n",
    "\n",
    "            print(f\"   {i:2d}. {col}\")\n",
    "            print(f\"       ├─ Tipo: {tipo}\")\n",
    "            print(f\"       ├─ Nulos: {nulos:,} ({pct_nulos:.1f}%)\")\n",
    "            print(f\"       └─ Únicos: {unicos:,}\")\n",
    "\n",
    "        print()\n",
    "\n",
    "        # ══════════════════════════════════════════════════════════════════\n",
    "        # ETAPA 5: Preview dos dados\n",
    "        # ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "        print(\"📊 PREVIEW DOS DADOS (primeiras 5 colunas)\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        colunas_preview = list(df.columns[:5])\n",
    "        print(df[colunas_preview].head(5).to_string(index=False))\n",
    "        print()\n",
    "\n",
    "        # ══════════════════════════════════════════════════════════════════\n",
    "        # ETAPA 6: Backup\n",
    "        # ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "        print(\"💾 Salvando backup...\")\n",
    "\n",
    "        timestamp = fm.timestamp\n",
    "        arquivo_destino = fm.diretorios['dados_entrada'] / \\\n",
    "                         f\"Centros_BR_{timestamp}.xlsx\"\n",
    "\n",
    "        try:\n",
    "            df.to_excel(arquivo_destino, index=False, engine='openpyxl')\n",
    "            fm.logger.info(f\"Backup: {arquivo_destino.name}\")\n",
    "            print(f\"   ✅ Backup salvo: {arquivo_destino.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️  Erro ao salvar backup: {str(e)}\")\n",
    "            fm.logger.error(f\"Erro backup: {str(e)}\")\n",
    "\n",
    "        print()\n",
    "\n",
    "        # ══════════════════════════════════════════════════════════════════\n",
    "        # RESULTADO FINAL\n",
    "        # ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "        print(\"╔\" + \"═\" * 78 + \"╗\")\n",
    "        print(\"║\" + \" ✅ ARQUIVO CENTROS CARREGADO COM SUCESSO \".center(78) + \"║\")\n",
    "        print(\"╚\" + \"═\" * 78 + \"╝\")\n",
    "        print()\n",
    "\n",
    "        print(\"📊 RESUMO DO CARREGAMENTO:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"   • Arquivo: {arquivo_path.name}\")\n",
    "        print(f\"   • Registros: {len(df):,}\")\n",
    "        print(f\"   • Colunas: {len(df.columns)}\")\n",
    "        print(f\"   • Backup: ✅\")\n",
    "        print()\n",
    "\n",
    "        return df, arquivo_path\n",
    "\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(\"╔\" + \"═\" * 78 + \"╗\")\n",
    "        print(\"║\" + \" ❌ ERRO AO CARREGAR ARQUIVO \".center(78) + \"║\")\n",
    "        print(\"╚\" + \"═\" * 78 + \"╝\")\n",
    "        print()\n",
    "        print(f\"❌ Erro: {str(e)}\")\n",
    "        print()\n",
    "\n",
    "        import traceback\n",
    "        print(\"🔍 Detalhes técnicos:\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "        fm.logger.error(f\"Erro centros: {str(e)}\")\n",
    "\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "# EXECUÇÃO: CARREGAR ARQUIVO 2\n",
    "# ══════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print()\n",
    "print(\"⚠️  INSTRUÇÕES:\")\n",
    "print(\"   • Janela de seleção vai abrir\")\n",
    "print(\"   • Selecione o arquivo ysmm_centros_br.xlsx\")\n",
    "print(\"   • Primeira linha deve ser o cabeçalho\")\n",
    "print()\n",
    "\n",
    "input(\"👉 Pressione ENTER para iniciar...\")\n",
    "\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # Carregar arquivo\n",
    "    df_centros, arquivo_centros = carregar_ysmm_centros()\n",
    "\n",
    "    if df_centros is not None:\n",
    "        # Salvar referência global\n",
    "        arquivo_centros_info = {\n",
    "            'df': df_centros,\n",
    "            'arquivo': arquivo_centros\n",
    "        }\n",
    "\n",
    "        print()\n",
    "        print(\"═\" * 80)\n",
    "        print(\"✅ BLOCO 3 - ARQUIVO 2 CONCLUÍDO\")\n",
    "        print(\"═\" * 80)\n",
    "        print()\n",
    "        print(\"📋 PRÓXIMOS PASSOS:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"   1. ✅ Revise a lista de colunas acima\")\n",
    "        print(\"   2. ✅ Verifique se os dados fazem sentido\")\n",
    "        print(\"   3. 📤 Envie feedback:\")\n",
    "        print(\"       • 'ARQUIVO 2 OK' - para prosseguir\")\n",
    "        print(\"       • Descreva problemas - se houver erros\")\n",
    "        print(\"   4. ⏳ Aguarde próximo arquivo\")\n",
    "        print()\n",
    "        print(\"🔗 VARIÁVEIS DISPONÍVEIS:\")\n",
    "        print(\"   • df_centros - DataFrame com cadastro de centros\")\n",
    "        print(\"   • arquivo_centros_info - Metadados completos\")\n",
    "        print()\n",
    "\n",
    "        fm.logger.info(\"BLOCO 3 - Arquivo 2 (Centros) concluído\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print()\n",
    "    print(\"⚠️  Operação cancelada pelo usuário\")\n",
    "\n",
    "except Exception as e:\n",
    "    print()\n",
    "    print(\"╔\" + \"═\" * 78 + \"╗\")\n",
    "    print(\"║\" + \" ERRO FATAL - BLOCO 3 \".center(78) + \"║\")\n",
    "    print(\"╚\" + \"═\" * 78 + \"╝\")\n",
    "    print()\n",
    "    print(f\"❌ Erro: {str(e)}\")\n",
    "    print()\n",
    "    import traceback\n",
    "    print(\"🔍 Detalhes técnicos:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print()\n",
    "print(\"═\" * 80)"
   ],
   "id": "232087a72dbd916",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "BLOCO 4: CARREGADOR AIVI OPAV BW - CORRIGIDO v3\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "Sheet correto: \"Valor da Variação Total\"\n",
    "Cabeçalho: L34-AM34 (APENAS, colunas AW-BH descartadas)\n",
    "Resultado: 28 colunas (7 dimensões + 21 movimentações)\n",
    "Tratamento dinâmico de duplicadas: sufixo _dup1, _dup2, etc\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "from pathlib import Path\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "print(\"BLOCO 4: CARREGADOR AIVI OPAV BW - ARQUIVO 3/N\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 1: Seleção do arquivo\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "if 'fm' in dir():\n",
    "    padrao_bw = '*xSAPtemp*.xls*'\n",
    "    arquivos_encontrados = list(fm.diretorios['dados_entrada'].glob(padrao_bw))\n",
    "\n",
    "    if arquivos_encontrados:\n",
    "        arquivo_path = arquivos_encontrados[0]\n",
    "        print(f\"Arquivo encontrado: {arquivo_path.name}\")\n",
    "    else:\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()\n",
    "        root.lift()\n",
    "        root.attributes('-topmost', True)\n",
    "\n",
    "        arquivo = filedialog.askopenfilename(\n",
    "            title=\"Selecione arquivo BW (xSAPtemp...)\",\n",
    "            filetypes=[(\"Excel\", \"*.xls *.xlsx\"), (\"All\", \"*.*\")]\n",
    "        )\n",
    "\n",
    "        root.destroy()\n",
    "\n",
    "        if not arquivo:\n",
    "            raise ValueError(\"Nenhum arquivo selecionado\")\n",
    "\n",
    "        arquivo_path = Path(arquivo)\n",
    "else:\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    root.lift()\n",
    "    root.attributes('-topmost', True)\n",
    "\n",
    "    arquivo = filedialog.askopenfilename(\n",
    "        title=\"Selecione arquivo BW (xSAPtemp...)\",\n",
    "        filetypes=[(\"Excel\", \"*.xls *.xlsx\"), (\"All\", \"*.*\")]\n",
    "    )\n",
    "\n",
    "    root.destroy()\n",
    "\n",
    "    if not arquivo:\n",
    "        raise ValueError(\"Nenhum arquivo selecionado\")\n",
    "\n",
    "    arquivo_path = Path(arquivo)\n",
    "\n",
    "print(f\"Arquivo selecionado: {arquivo_path.name}\")\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 2: Carregar SHEET CORRETO\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"ETAPA 2: Carregando sheet correto...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Nome do sheet correto\n",
    "SHEET_NAME = \"Valor da Variação Total\"\n",
    "\n",
    "try:\n",
    "    # Método 1: xlrd (mais confiável para .xls)\n",
    "    print(f\"Tentando xlrd (sheet: '{SHEET_NAME}')...\")\n",
    "\n",
    "    workbook = xlrd.open_workbook(str(arquivo_path))\n",
    "\n",
    "    # Listar sheets\n",
    "    print(f\"   Sheets disponiveis: {workbook.sheet_names()}\")\n",
    "\n",
    "    # Verificar se sheet existe\n",
    "    if SHEET_NAME not in workbook.sheet_names():\n",
    "        raise ValueError(f\"Sheet '{SHEET_NAME}' nao encontrado!\")\n",
    "\n",
    "    # Pegar sheet correto\n",
    "    sheet = workbook.sheet_by_name(SHEET_NAME)\n",
    "\n",
    "    print(f\"   Sheet: '{SHEET_NAME}'\")\n",
    "    print(f\"   Linhas: {sheet.nrows:,}\")\n",
    "    print(f\"   Colunas: {sheet.ncols}\")\n",
    "\n",
    "    # Converter para lista de listas\n",
    "    data = []\n",
    "    for row_idx in range(sheet.nrows):\n",
    "        data.append(sheet.row_values(row_idx))\n",
    "\n",
    "    # Criar DataFrame\n",
    "    df_bruto = pd.DataFrame(data)\n",
    "\n",
    "    print(f\"   DataFrame criado: {df_bruto.shape[0]:,} x {df_bruto.shape[1]}\")\n",
    "    print()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   Erro com xlrd: {str(e)}\")\n",
    "    print()\n",
    "\n",
    "    # Método 2: pandas como fallback\n",
    "    print(f\"Tentando pandas (sheet: '{SHEET_NAME}')...\")\n",
    "\n",
    "    df_bruto = pd.read_excel(\n",
    "        arquivo_path,\n",
    "        sheet_name=SHEET_NAME,\n",
    "        header=None\n",
    "    )\n",
    "\n",
    "    print(f\"   DataFrame criado: {df_bruto.shape[0]:,} x {df_bruto.shape[1]}\")\n",
    "    print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 3: VERIFICAR LINHA 34 (índice 33)\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"ETAPA 3: Verificando linha 34 (cabecalho)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Linha 34 Excel = índice 33 pandas\n",
    "linha_34 = df_bruto.iloc[33]\n",
    "\n",
    "# Verificar colunas L-BH (índices 11-59)\n",
    "cols_lbh = linha_34.iloc[11:60]\n",
    "\n",
    "# Contar não-nulas\n",
    "non_null = cols_lbh.notna().sum()\n",
    "print(f\"   Celulas nao-nulas em L34-BH34: {non_null}/49\")\n",
    "\n",
    "# Mostrar primeiras não-nulas\n",
    "print(f\"   Primeiros valores nao-nulos:\")\n",
    "count = 0\n",
    "for i, val in enumerate(cols_lbh):\n",
    "    if pd.notna(val) and count < 10:\n",
    "        col_idx = 11 + i\n",
    "        # Nome da coluna (L=11, M=12, etc)\n",
    "        if col_idx < 26:\n",
    "            col_name = chr(65 + col_idx)\n",
    "        else:\n",
    "            col_name = chr(65 + (col_idx // 26) - 1) + chr(65 + (col_idx % 26))\n",
    "\n",
    "        val_str = str(val)[:30]\n",
    "        print(f\"      {col_name}34 (idx={col_idx}): '{val_str}'\")\n",
    "        count += 1\n",
    "\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 4: EXTRAIR CABEÇALHO CORRETO\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"ETAPA 4: Extraindo cabecalho das colunas especificas...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Linha 34 Excel = índice 33\n",
    "linha_cabecalho = df_bruto.iloc[33]\n",
    "\n",
    "# APENAS PARTE 1: Colunas L-AM (índices 11-38)\n",
    "# L=11, M=12, ..., AM=38 (28 colunas)\n",
    "cabecalho_bruto = linha_cabecalho.iloc[11:39].tolist()\n",
    "\n",
    "print(f\"   COLUNAS EXTRAIDAS (L34-AM34): {len(cabecalho_bruto)} colunas\")\n",
    "print()\n",
    "\n",
    "# Limpar nomes\n",
    "cabecalho_temp = []\n",
    "for i, nome in enumerate(cabecalho_bruto):\n",
    "    if pd.isna(nome) or str(nome).strip() == '':\n",
    "        nome_limpo = f'Col_{i}'\n",
    "    else:\n",
    "        # Remover apóstrofo inicial se existir\n",
    "        nome_str = str(nome).strip()\n",
    "        if nome_str.startswith(\"'\"):\n",
    "            nome_str = nome_str[1:]\n",
    "        # Remover quebras de linha\n",
    "        nome_str = nome_str.replace('\\n', ' ')\n",
    "        nome_limpo = nome_str\n",
    "    cabecalho_temp.append(nome_limpo)\n",
    "\n",
    "# Tratar duplicadas (adicionar sufixo _dup1, _dup2, etc)\n",
    "from collections import Counter\n",
    "\n",
    "contagem = Counter(cabecalho_temp)\n",
    "duplicadas = {nome: count for nome, count in contagem.items() if count > 1}\n",
    "\n",
    "if duplicadas:\n",
    "    print(f\"   AVISO: {len(duplicadas)} nomes duplicados encontrados\")\n",
    "    for nome, count in list(duplicadas.items())[:5]:\n",
    "        print(f\"      '{nome}': {count} ocorrencias\")\n",
    "    print()\n",
    "\n",
    "# Renomear duplicadas\n",
    "cabecalho_limpo = []\n",
    "contador = {}\n",
    "\n",
    "for nome in cabecalho_temp:\n",
    "    if nome in contador:\n",
    "        contador[nome] += 1\n",
    "        novo_nome = f\"{nome}_dup{contador[nome]}\"\n",
    "        cabecalho_limpo.append(novo_nome)\n",
    "    else:\n",
    "        contador[nome] = 0\n",
    "        cabecalho_limpo.append(nome)\n",
    "\n",
    "# Mostrar cabeçalho final\n",
    "print(\"   Cabecalho final:\")\n",
    "for i, nome in enumerate(cabecalho_limpo[:10], 1):\n",
    "    print(f\"      A{i if i <= 9 else str(i)}: {nome}\")\n",
    "if len(cabecalho_limpo) > 10:\n",
    "    print(f\"      ... (mais {len(cabecalho_limpo) - 10} colunas)\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 5: EXTRAIR DADOS (linhas após linha 34)\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"ETAPA 5: Extraindo dados...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Dados começam na linha 35 (índice 34)\n",
    "linha_inicio_dados = 34\n",
    "\n",
    "# Extrair APENAS colunas L-AM (índices 11-38)\n",
    "df_final = df_bruto.iloc[linha_inicio_dados:, 11:39].copy()\n",
    "\n",
    "# Aplicar cabeçalho\n",
    "df_final.columns = cabecalho_limpo\n",
    "\n",
    "# Reset index\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "print(f\"   Registros: {len(df_final):,}\")\n",
    "print(f\"   Colunas: {len(df_final.columns)}\")\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 6: CONVERTER DIMENSÕES PARA STRING\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"ETAPA 6: Convertendo dimensoes para STRING...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Primeiras 7 colunas = dimensões\n",
    "dimensoes_cols = df_final.columns[:7].tolist()\n",
    "\n",
    "for col in dimensoes_cols:\n",
    "    if col in df_final.columns:\n",
    "        df_final[col] = df_final[col].astype(str)\n",
    "        print(f\"   '{col}' -> STRING\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 7: PREVIEW DOS DADOS\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"ETAPA 7: PREVIEW DOS DADOS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Primeiras 7 colunas (DIMENSOES):\")\n",
    "print(df_final.iloc[:, :7].head(5).to_string(index=False))\n",
    "print()\n",
    "\n",
    "print(\"Primeiras 5 linhas completas:\")\n",
    "print(df_final.head(5).to_string(index=False, max_colwidth=20))\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 8: ESTATÍSTICAS DAS COLUNAS\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"ETAPA 8: ESTATISTICAS DAS COLUNAS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, col in enumerate(df_final.columns, 1):\n",
    "    # Tratar caso de coluna duplicada (retorna DataFrame)\n",
    "    try:\n",
    "        col_serie = df_final[col]\n",
    "\n",
    "        # Se retornar DataFrame (duplicada), pegar primeira coluna\n",
    "        if isinstance(col_serie, pd.DataFrame):\n",
    "            col_serie = col_serie.iloc[:, 0]\n",
    "\n",
    "        tipo = col_serie.dtype\n",
    "        nulos = col_serie.isna().sum()\n",
    "        pct_nulos = (nulos / len(df_final) * 100) if len(df_final) > 0 else 0\n",
    "        unicos = col_serie.nunique()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Fallback se der erro\n",
    "        tipo = \"unknown\"\n",
    "        nulos = 0\n",
    "        pct_nulos = 0.0\n",
    "        unicos = 0\n",
    "        print(f\"   AVISO: Erro ao analisar coluna '{col}': {str(e)[:50]}\")\n",
    "\n",
    "    # Identificar bloco\n",
    "    if i <= 7:\n",
    "        bloco = \"DIMENSAO\"\n",
    "    elif i <= 28:\n",
    "        bloco = \"MOVIM_BLK1\"\n",
    "    else:\n",
    "        bloco = \"MOVIM_BLK2\"\n",
    "\n",
    "    print(f\"   {i:2d}. {col[:30]}\")\n",
    "    print(f\"       Bloco: {bloco} | Tipo: {tipo} | Nulos: {pct_nulos:.1f}% | Unicos: {unicos:,}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 9: SALVAR BACKUP\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"ETAPA 9: Salvando backup...\")\n",
    "\n",
    "if 'fm' in dir():\n",
    "    timestamp = fm.timestamp\n",
    "    arquivo_destino = fm.diretorios['dados_entrada'] / f\"AIVI_OPAV_BW_{timestamp}.xlsx\"\n",
    "\n",
    "    try:\n",
    "        df_final.to_excel(arquivo_destino, index=False, engine='openpyxl')\n",
    "        fm.logger.info(f\"Backup BW: {arquivo_destino.name}\")\n",
    "        print(f\"   Backup: {arquivo_destino.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Erro ao salvar: {str(e)}\")\n",
    "        fm.logger.error(f\"Erro backup BW: {str(e)}\")\n",
    "else:\n",
    "    print(\"   FileManager nao disponivel - backup ignorado\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# RESULTADO FINAL\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ARQUIVO BW CARREGADO E PROCESSADO COM SUCESSO\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"RESUMO DO PROCESSAMENTO:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Arquivo: {arquivo_path.name}\")\n",
    "print(f\"   Sheet: {SHEET_NAME}\")\n",
    "print(f\"   Registros: {len(df_final):,}\")\n",
    "print(f\"   Colunas totais: {len(df_final.columns)}\")\n",
    "print(f\"   Dimensoes (1-7): 7\")\n",
    "print(f\"   Movimentacoes (8-{len(df_final.columns)}): {len(df_final.columns) - 7}\")\n",
    "print()\n",
    "\n",
    "# Salvar na variável global\n",
    "df_opav = df_final\n",
    "arquivo_opav = arquivo_path\n",
    "\n",
    "print(\"VARIAVEIS DISPONIVEIS:\")\n",
    "print(\"   df_opav - DataFrame BW processado\")\n",
    "print(\"   arquivo_opav - Path do arquivo\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FIM BLOCO 4\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "fc881bbc1ead00a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "BLOCO 5: UNIFICADOR DE ARQUIVOS OPAV BW\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "Busca recursiva: *xSAPtemp*.xls* em pasta e subpastas\n",
    "Carrega cada arquivo com mesmo padrão do BLOCO 4\n",
    "Unifica tudo em um único DataFrame\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "print(\"╔\" + \"═\" * 78 + \"╗\")\n",
    "print(\"║\" + \" BLOCO 5: UNIFICADOR DE ARQUIVOS OPAV BW \".center(78) + \"║\")\n",
    "print(\"╚\" + \"═\" * 78 + \"╝\")\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# CONFIGURAÇÕES\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "SHEET_NAME = \"Valor da Variação Total\"\n",
    "LINHA_CABECALHO = 33  # Linha 34 do Excel\n",
    "LINHA_INICIO_DADOS = 34\n",
    "\n",
    "# Colunas a extrair (APENAS L-AM, 28 colunas)\n",
    "COLS_EXTRAIR = (11, 39)  # L-AM (índices 11-38)\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# FUNÇÕES AUXILIARES (do BLOCO 4)\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "def limpar_nome_coluna(nome):\n",
    "    \"\"\"\n",
    "    Limpa nome de coluna de caracteres especiais\n",
    "    \"\"\"\n",
    "    if pd.isna(nome) or str(nome).strip() == '':\n",
    "        return None\n",
    "\n",
    "    nome_str = str(nome).strip()\n",
    "    nome_str = nome_str.lstrip(\"'\")\n",
    "    nome_str = nome_str.replace('\\n', ' ')\n",
    "    nome_str = nome_str.replace('\\r', ' ')\n",
    "    nome_str = ' '.join(nome_str.split())\n",
    "\n",
    "    return nome_str\n",
    "\n",
    "def tratar_duplicadas(colunas):\n",
    "    \"\"\"\n",
    "    Adiciona sufixo _dup1, _dup2 às duplicadas\n",
    "    \"\"\"\n",
    "    contagem = Counter(colunas)\n",
    "    duplicadas = {n: c for n, c in contagem.items() if c > 1}\n",
    "\n",
    "    if duplicadas:\n",
    "        print(f\"   AVISO: {len(duplicadas)} nomes duplicados\")\n",
    "        for nome, count in list(duplicadas.items())[:3]:\n",
    "            print(f\"      '{nome}': {count} ocorrencias\")\n",
    "\n",
    "    colunas_novas = []\n",
    "    contador = {}\n",
    "\n",
    "    for nome in colunas:\n",
    "        if nome in contador:\n",
    "            contador[nome] += 1\n",
    "            colunas_novas.append(f\"{nome}_dup{contador[nome]}\")\n",
    "        else:\n",
    "            contador[nome] = 0\n",
    "            colunas_novas.append(nome)\n",
    "\n",
    "    return colunas_novas\n",
    "\n",
    "def carregar_arquivo_opav(arquivo_path):\n",
    "    \"\"\"\n",
    "    Carrega um arquivo OPAV seguindo padrão do BLOCO 4\n",
    "\n",
    "    Retorna: DataFrame ou None se erro\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\n📄 Processando: {arquivo_path.name}\")\n",
    "        print(\"   \" + \"-\" * 76)\n",
    "\n",
    "        # Carregar com xlrd\n",
    "        workbook = xlrd.open_workbook(str(arquivo_path))\n",
    "\n",
    "        # Verificar sheet\n",
    "        if SHEET_NAME not in workbook.sheet_names():\n",
    "            print(f\"   ❌ Sheet '{SHEET_NAME}' nao encontrado\")\n",
    "            print(f\"   Sheets disponiveis: {workbook.sheet_names()}\")\n",
    "            return None\n",
    "\n",
    "        sheet = workbook.sheet_by_name(SHEET_NAME)\n",
    "        print(f\"   ✅ Sheet: '{SHEET_NAME}' ({sheet.nrows} linhas × {sheet.ncols} cols)\")\n",
    "\n",
    "        # Converter para DataFrame\n",
    "        data = [sheet.row_values(i) for i in range(sheet.nrows)]\n",
    "        df_bruto = pd.DataFrame(data)\n",
    "\n",
    "        # Extrair cabeçalho (APENAS L-AM)\n",
    "        linha_cab = df_bruto.iloc[LINHA_CABECALHO]\n",
    "        cab_bruto = linha_cab.iloc[COLS_EXTRAIR[0]:COLS_EXTRAIR[1]].tolist()\n",
    "\n",
    "        # Limpar cabeçalho\n",
    "        cab_temp = [limpar_nome_coluna(c) or f'Col_{i}'\n",
    "                    for i, c in enumerate(cab_bruto)]\n",
    "\n",
    "        # Tratar duplicadas\n",
    "        cab_final = tratar_duplicadas(cab_temp)\n",
    "\n",
    "        # Extrair dados (APENAS L-AM)\n",
    "        df_final = df_bruto.iloc[LINHA_INICIO_DADOS:, COLS_EXTRAIR[0]:COLS_EXTRAIR[1]].copy()\n",
    "\n",
    "        df_final.columns = cab_final\n",
    "        df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "        # Adicionar metadados\n",
    "        df_final['_arquivo_origem'] = arquivo_path.name\n",
    "        df_final['_arquivo_path'] = str(arquivo_path)\n",
    "        df_final['_data_carga'] = pd.Timestamp.now()\n",
    "\n",
    "        print(f\"   ✅ Carregado: {len(df_final):,} registros × {len(cab_final)} colunas\")\n",
    "\n",
    "        return df_final\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ ERRO: {str(e)[:100]}\")\n",
    "        import traceback\n",
    "        print(f\"   Detalhes: {traceback.format_exc()[:200]}\")\n",
    "        return None\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 1: Determinar pasta base\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"ETAPA 1: Determinar pasta com arquivos OPAV\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "\n",
    "pasta_base = None\n",
    "\n",
    "# OPÇÃO 1: Usar pasta do arquivo já carregado (BLOCO 4)\n",
    "if 'arquivo_opav' in dir():\n",
    "    pasta_arquivo = arquivo_opav.parent\n",
    "    print(f\"📁 Arquivo OPAV ja carregado no BLOCO 4:\")\n",
    "    print(f\"   Arquivo: {arquivo_opav.name}\")\n",
    "    print(f\"   Pasta: {pasta_arquivo}\")\n",
    "    print()\n",
    "\n",
    "    # Verificar se há outros arquivos nessa pasta\n",
    "    outros_arquivos = list(pasta_arquivo.glob('*xSAPtemp*.xls*'))\n",
    "\n",
    "    if len(outros_arquivos) > 1:\n",
    "        print(f\"   ✅ Encontrados {len(outros_arquivos)} arquivos xSAPtemp nesta pasta\")\n",
    "        print(f\"   Usando esta pasta como base\")\n",
    "        pasta_base = pasta_arquivo\n",
    "    elif len(outros_arquivos) == 1:\n",
    "        print(f\"   ⚠️  Apenas 1 arquivo xSAPtemp nesta pasta\")\n",
    "        print(f\"   Vou perguntar se quer buscar em outra pasta\")\n",
    "    else:\n",
    "        print(f\"   ⚠️  Nenhum arquivo xSAPtemp nesta pasta\")\n",
    "        print(f\"   Vou perguntar outra pasta\")\n",
    "\n",
    "# OPÇÃO 2: Perguntar ao usuário\n",
    "if pasta_base is None:\n",
    "    print()\n",
    "    print(\"═\" * 80)\n",
    "    print(\"📂 SELEÇÃO DE PASTA\")\n",
    "    print(\"═\" * 80)\n",
    "    print()\n",
    "    print(\"Opcoes:\")\n",
    "    print(\"   1. Selecionar pasta manualmente\")\n",
    "    if 'arquivo_opav' in dir():\n",
    "        print(f\"   2. Usar pasta do arquivo atual ({arquivo_opav.parent.name})\")\n",
    "    if 'fm' in dir():\n",
    "        print(f\"   3. Usar pasta do FileManager ({fm.diretorios['dados_entrada'].name})\")\n",
    "    print()\n",
    "\n",
    "    escolha = input(\"Escolha (1/2/3): \").strip()\n",
    "\n",
    "    if escolha == '1':\n",
    "        # Janela de seleção\n",
    "        print(\"\\nAbrindo janela de selecao de pasta...\")\n",
    "\n",
    "        root = tk.Tk()\n",
    "        root.withdraw()\n",
    "        root.lift()\n",
    "        root.attributes('-topmost', True)\n",
    "\n",
    "        pasta = filedialog.askdirectory(\n",
    "            title=\"Selecione pasta com arquivos OPAV (xSAPtemp...)\"\n",
    "        )\n",
    "\n",
    "        root.destroy()\n",
    "\n",
    "        if not pasta:\n",
    "            raise ValueError(\"Nenhuma pasta selecionada\")\n",
    "\n",
    "        pasta_base = Path(pasta)\n",
    "\n",
    "    elif escolha == '2' and 'arquivo_opav' in dir():\n",
    "        pasta_base = arquivo_opav.parent\n",
    "\n",
    "    elif escolha == '3' and 'fm' in dir():\n",
    "        pasta_base = fm.diretorios['dados_entrada']\n",
    "\n",
    "    else:\n",
    "        # Default: FileManager ou erro\n",
    "        if 'fm' in dir():\n",
    "            pasta_base = fm.diretorios['dados_entrada']\n",
    "        else:\n",
    "            raise ValueError(\"Opcao invalida\")\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 1.5: Copiar arquivo para pasta FileManager (se necessário)\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "if 'arquivo_opav' in dir() and 'fm' in dir():\n",
    "    # Verificar se arquivo está fora da pasta do FileManager\n",
    "    arquivo_atual = arquivo_opav\n",
    "    pasta_fm = fm.diretorios['dados_entrada']\n",
    "\n",
    "    if arquivo_atual.parent != pasta_fm:\n",
    "        print()\n",
    "        print(\"═\" * 80)\n",
    "        print(\"📋 ARQUIVO FORA DA PASTA DO FILEMANAGER\")\n",
    "        print(\"═\" * 80)\n",
    "        print()\n",
    "        print(f\"Arquivo atual: {arquivo_atual}\")\n",
    "        print(f\"Pasta FileManager: {pasta_fm}\")\n",
    "        print()\n",
    "\n",
    "        # Verificar tamanho\n",
    "        tamanho_mb = arquivo_atual.stat().st_size / (1024 * 1024)\n",
    "        print(f\"Tamanho do arquivo: {tamanho_mb:.2f} MB\")\n",
    "        print()\n",
    "\n",
    "        if tamanho_mb > 50:\n",
    "            print(\"⚠️  Arquivo grande (>50MB)\")\n",
    "            print()\n",
    "            print(\"Opcoes:\")\n",
    "            print(\"   1. Copiar para pasta FileManager (pode demorar)\")\n",
    "            print(\"   2. Buscar na pasta atual do arquivo\")\n",
    "            print(\"   3. Selecionar outra pasta\")\n",
    "            print()\n",
    "\n",
    "            escolha_copia = input(\"Escolha (1/2/3): \").strip()\n",
    "\n",
    "            if escolha_copia == '1':\n",
    "                print(\"\\n📋 Copiando arquivo...\")\n",
    "                import shutil\n",
    "\n",
    "                destino = pasta_fm / arquivo_atual.name\n",
    "\n",
    "                try:\n",
    "                    shutil.copy2(arquivo_atual, destino)\n",
    "                    print(f\"   ✅ Copiado para: {destino}\")\n",
    "                    print()\n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Erro ao copiar: {str(e)}\")\n",
    "                    print(f\"   Usando pasta original\")\n",
    "                    print()\n",
    "\n",
    "            elif escolha_copia == '2':\n",
    "                print(\"\\n✅ Usando pasta do arquivo atual\")\n",
    "                print()\n",
    "\n",
    "            elif escolha_copia == '3':\n",
    "                print(\"\\n📂 Abrindo janela de selecao...\")\n",
    "                root = tk.Tk()\n",
    "                root.withdraw()\n",
    "                root.lift()\n",
    "                root.attributes('-topmost', True)\n",
    "\n",
    "                pasta = filedialog.askdirectory(\n",
    "                    title=\"Selecione pasta com arquivos OPAV\"\n",
    "                )\n",
    "\n",
    "                root.destroy()\n",
    "\n",
    "                if pasta:\n",
    "                    pasta_base = Path(pasta)\n",
    "                    print(f\"   ✅ Pasta selecionada: {pasta_base}\")\n",
    "                print()\n",
    "        else:\n",
    "            # Arquivo pequeno (<50MB), copiar automaticamente\n",
    "            print(\"✅ Arquivo pequeno (<50MB)\")\n",
    "            print(\"   Copiando para pasta FileManager...\")\n",
    "\n",
    "            import shutil\n",
    "            destino = pasta_fm / arquivo_atual.name\n",
    "\n",
    "            try:\n",
    "                shutil.copy2(arquivo_atual, destino)\n",
    "                print(f\"   ✅ Copiado para: {destino.name}\")\n",
    "                print()\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  Erro ao copiar: {str(e)}\")\n",
    "                print(f\"   Continuando com pasta original\")\n",
    "                print()\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"✅ PASTA BASE DEFINIDA: {pasta_base}\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 2: Buscar arquivos recursivamente\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"ETAPA 2: Buscando arquivos xSAPtemp recursivamente...\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "\n",
    "# Buscar com glob recursivo\n",
    "padroes = ['*xSAPtemp*.xls', '*xSAPtemp*.xlsx']\n",
    "arquivos_encontrados = []\n",
    "\n",
    "for padrao in padroes:\n",
    "    arquivos_encontrados.extend(pasta_base.rglob(padrao))\n",
    "\n",
    "# Remover duplicatas (caso encontre mesmo arquivo com ambos padrões)\n",
    "arquivos_encontrados = list(set(arquivos_encontrados))\n",
    "\n",
    "# Ordenar por nome\n",
    "arquivos_encontrados.sort(key=lambda x: x.name)\n",
    "\n",
    "print(f\"✅ Encontrados: {len(arquivos_encontrados)} arquivos\")\n",
    "print()\n",
    "\n",
    "if len(arquivos_encontrados) == 0:\n",
    "    print(\"❌ NENHUM ARQUIVO ENCONTRADO!\")\n",
    "    print()\n",
    "    print(\"Padrões buscados:\")\n",
    "    for padrao in padroes:\n",
    "        print(f\"   • {padrao}\")\n",
    "    print()\n",
    "    print(f\"Pasta base: {pasta_base}\")\n",
    "    print()\n",
    "    raise ValueError(\"Nenhum arquivo xSAPtemp encontrado\")\n",
    "\n",
    "# Listar arquivos encontrados\n",
    "print(\"📋 ARQUIVOS ENCONTRADOS:\")\n",
    "print(\"-\" * 80)\n",
    "for i, arq in enumerate(arquivos_encontrados, 1):\n",
    "    # Caminho relativo à pasta base\n",
    "    rel_path = arq.relative_to(pasta_base)\n",
    "    tamanho_mb = arq.stat().st_size / (1024 * 1024)\n",
    "    print(f\"   {i:2d}. {arq.name}\")\n",
    "    print(f\"       Pasta: {rel_path.parent}\")\n",
    "    print(f\"       Tamanho: {tamanho_mb:.2f} MB\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Confirmação do usuário\n",
    "print(\"═\" * 80)\n",
    "print(\"⚠️  CONFIRMAÇÃO NECESSÁRIA\")\n",
    "print(\"═\" * 80)\n",
    "print()\n",
    "print(f\"Foram encontrados {len(arquivos_encontrados)} arquivos.\")\n",
    "print(\"Todos serão processados e unificados em um único DataFrame.\")\n",
    "print()\n",
    "print(\"Deseja prosseguir?\")\n",
    "print(\"   [ENTER] = SIM, continuar\")\n",
    "print(\"   [Ctrl+C] = NÃO, cancelar\")\n",
    "print()\n",
    "\n",
    "input(\"Pressione ENTER para continuar...\")\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 3: Carregar todos os arquivos\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"ETAPA 3: Carregando todos os arquivos...\")\n",
    "print(\"═\" * 80)\n",
    "\n",
    "dataframes = []\n",
    "erros = []\n",
    "\n",
    "for i, arquivo in enumerate(arquivos_encontrados, 1):\n",
    "    print(f\"\\n[{i}/{len(arquivos_encontrados)}] Processando...\")\n",
    "\n",
    "    df = carregar_arquivo_opav(arquivo)\n",
    "\n",
    "    if df is not None:\n",
    "        dataframes.append(df)\n",
    "    else:\n",
    "        erros.append(arquivo.name)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(f\"RESUMO DO CARREGAMENTO:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"   ✅ Sucesso: {len(dataframes)} arquivos\")\n",
    "print(f\"   ❌ Erros: {len(erros)} arquivos\")\n",
    "\n",
    "if erros:\n",
    "    print(f\"\\n   Arquivos com erro:\")\n",
    "    for erro in erros:\n",
    "        print(f\"      • {erro}\")\n",
    "\n",
    "print()\n",
    "\n",
    "if len(dataframes) == 0:\n",
    "    raise ValueError(\"Nenhum arquivo foi carregado com sucesso!\")\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 4: Unificar DataFrames\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"ETAPA 4: Unificando DataFrames...\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "\n",
    "# Concatenar verticalmente\n",
    "df_unificado = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "print(f\"✅ DataFrame unificado criado\")\n",
    "print(f\"   Registros totais: {len(df_unificado):,}\")\n",
    "print(f\"   Colunas: {len(df_unificado.columns)}\")\n",
    "print()\n",
    "\n",
    "# Estatísticas por arquivo\n",
    "print(\"📊 REGISTROS POR ARQUIVO:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "contagem_por_arquivo = df_unificado['_arquivo_origem'].value_counts().sort_index()\n",
    "\n",
    "for arquivo, count in contagem_por_arquivo.items():\n",
    "    pct = (count / len(df_unificado)) * 100\n",
    "    print(f\"   {arquivo}: {count:,} registros ({pct:.1f}%)\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 5: Validações\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"ETAPA 5: Validações...\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "\n",
    "# Verificar duplicatas\n",
    "print(\"🔍 Verificando duplicatas...\")\n",
    "\n",
    "# Colunas chave (sem metadados)\n",
    "colunas_chave = [c for c in df_unificado.columns\n",
    "                 if not c.startswith('_')][:7]\n",
    "\n",
    "print(f\"   Colunas chave: {colunas_chave[:5]}...\")\n",
    "\n",
    "duplicatas = df_unificado.duplicated(subset=colunas_chave)\n",
    "n_duplicatas = duplicatas.sum()\n",
    "\n",
    "if n_duplicatas > 0:\n",
    "    print(f\"   ⚠️  {n_duplicatas:,} linhas duplicadas encontradas\")\n",
    "    print(f\"   Mantendo primeira ocorrência...\")\n",
    "    df_unificado = df_unificado[~duplicatas].reset_index(drop=True)\n",
    "    print(f\"   ✅ Após remover: {len(df_unificado):,} registros\")\n",
    "else:\n",
    "    print(f\"   ✅ Nenhuma duplicata encontrada\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Valores nulos em colunas críticas\n",
    "print(\"🔍 Verificando nulos em colunas críticas...\")\n",
    "\n",
    "colunas_criticas = ['Centro', 'Produto', 'Ano civil/mês']\n",
    "\n",
    "for col in colunas_criticas:\n",
    "    if col in df_unificado.columns:\n",
    "        nulos = df_unificado[col].isna().sum()\n",
    "        if nulos > 0:\n",
    "            print(f\"   ⚠️  '{col}': {nulos:,} nulos\")\n",
    "        else:\n",
    "            print(f\"   ✅ '{col}': 0 nulos\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 6: Preview e Estatísticas\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"ETAPA 6: PREVIEW E ESTATISTICAS\")\n",
    "print(\"═\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"Primeiras 7 colunas (DIMENSOES):\")\n",
    "colunas_dim = [c for c in df_unificado.columns if not c.startswith('_')][:7]\n",
    "print(df_unificado[colunas_dim].head(5).to_string(index=False))\n",
    "print()\n",
    "\n",
    "print(\"Ultimas 3 colunas (METADADOS):\")\n",
    "print(df_unificado[['_arquivo_origem', '_arquivo_path', '_data_carga']].head(3).to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Estatísticas resumidas\n",
    "print(\"📊 ESTATISTICAS GERAIS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"   Registros totais: {len(df_unificado):,}\")\n",
    "print(f\"   Colunas totais: {len(df_unificado.columns)}\")\n",
    "print(f\"   Arquivos unificados: {len(dataframes)}\")\n",
    "print(f\"   Período de carga: {df_unificado['_data_carga'].min()} a {df_unificado['_data_carga'].max()}\")\n",
    "\n",
    "# Dimensões únicas\n",
    "if 'Centro' in df_unificado.columns:\n",
    "    print(f\"   Centros únicos: {df_unificado['Centro'].nunique()}\")\n",
    "\n",
    "if 'Produto' in df_unificado.columns:\n",
    "    print(f\"   Produtos únicos: {df_unificado['Produto'].nunique()}\")\n",
    "\n",
    "if 'Ano civil/mês' in df_unificado.columns:\n",
    "    print(f\"   Períodos únicos: {df_unificado['Ano civil/mês'].nunique()}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# ETAPA 7: Salvar Backup\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"ETAPA 7: Salvando backup...\")\n",
    "print(\"-\" * 80)\n",
    "print()\n",
    "\n",
    "if 'fm' in dir():\n",
    "    timestamp = fm.timestamp\n",
    "    arquivo_destino = fm.diretorios['dados_processados'] / f\"AIVI_OPAV_UNIFICADO_{timestamp}.xlsx\"\n",
    "\n",
    "    try:\n",
    "        df_unificado.to_excel(arquivo_destino, index=False, engine='openpyxl')\n",
    "        fm.logger.info(f\"OPAV unificado: {arquivo_destino.name} ({len(df_unificado)} registros)\")\n",
    "        print(f\"   ✅ Backup: {arquivo_destino.name}\")\n",
    "        print(f\"   📁 Local: {arquivo_destino.parent}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ⚠️  Erro ao salvar: {str(e)}\")\n",
    "        fm.logger.error(f\"Erro backup OPAV unificado: {str(e)}\")\n",
    "else:\n",
    "    print(\"   ⚠️  FileManager não disponível - backup ignorado\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "# RESULTADO FINAL\n",
    "# ══════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"╔\" + \"═\" * 78 + \"╗\")\n",
    "print(\"║\" + \" ✅ UNIFICAÇÃO CONCLUÍDA COM SUCESSO \".center(78) + \"║\")\n",
    "print(\"╚\" + \"═\" * 78 + \"╝\")\n",
    "print()\n",
    "\n",
    "print(\"RESUMO FINAL:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   📄 Arquivos processados: {len(dataframes)}/{len(arquivos_encontrados)}\")\n",
    "print(f\"   📊 Registros totais: {len(df_unificado):,}\")\n",
    "print(f\"   📋 Colunas: {len(df_unificado.columns)}\")\n",
    "print(f\"   🗂️  Estrutura:\")\n",
    "print(f\"       • Dimensões: 7\")\n",
    "print(f\"       • Movimentações: {len(df_unificado.columns) - 10}\")\n",
    "print(f\"       • Metadados: 3\")\n",
    "print()\n",
    "\n",
    "# Salvar na variável global\n",
    "df_opav_unificado = df_unificado\n",
    "arquivos_opav_processados = [arq.name for arq in arquivos_encontrados if carregar_arquivo_opav(arq) is not None]\n",
    "\n",
    "print(\"VARIAVEIS DISPONIVEIS:\")\n",
    "print(\"   df_opav_unificado - DataFrame unificado\")\n",
    "print(\"   arquivos_opav_processados - Lista de arquivos\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FIM BLOCO 5\")\n",
    "print(\"=\" * 80)"
   ],
   "id": "122aa20ef772bcdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e7b9900a4c7e6c9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# BLOCO 0A: DETECTOR DE ARQUIVO DESCONHECIDO\n",
    "# Sistema inteligente para processar arquivos Excel de estrutura desconhecida\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class DetectorArquivoDesconhecido:\n",
    "    \"\"\"\n",
    "    Sistema para detectar automaticamente estrutura de arquivos Excel\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, arquivo_path, fm=None):\n",
    "        self.arquivo_path = Path(arquivo_path)\n",
    "        self.fm = fm\n",
    "        self.nome_arquivo = self.arquivo_path.name\n",
    "        self.workbook = None\n",
    "        self.sheet_detectada = None\n",
    "        self.linha_cabecalho = None\n",
    "        self.linha_dados_inicio = None\n",
    "        self.df_bruto = None\n",
    "        self.df_limpo = None\n",
    "        self.log = []\n",
    "\n",
    "    def processar(self):\n",
    "        \"\"\"\n",
    "        Processamento completo do arquivo\n",
    "        \"\"\"\n",
    "        self._log(\"🔍 INICIANDO DETECÇÃO AUTOMÁTICA\", nivel=\"TITULO\")\n",
    "        self._log(f\"📁 Arquivo: {self.nome_arquivo}\")\n",
    "\n",
    "        # 1. Carregar workbook\n",
    "        self._carregar_workbook()\n",
    "\n",
    "        # 2. Detectar sheet correto\n",
    "        self._detectar_sheet()\n",
    "\n",
    "        # 3. Detectar linha de cabeçalho\n",
    "        self._detectar_cabecalho()\n",
    "\n",
    "        # 4. Extrair e limpar dados\n",
    "        self._extrair_dados()\n",
    "\n",
    "        # 5. Limpar estrutura\n",
    "        self._limpar_estrutura()\n",
    "\n",
    "        # 6. Relatório final\n",
    "        self._gerar_relatorio()\n",
    "\n",
    "        return self.df_limpo\n",
    "\n",
    "    def _carregar_workbook(self):\n",
    "        \"\"\"\n",
    "        Carrega workbook Excel\n",
    "        \"\"\"\n",
    "        self._log(\"\\n📂 FASE 1: Carregamento do Arquivo\", nivel=\"SECAO\")\n",
    "\n",
    "        try:\n",
    "            # Tentar xlrd primeiro (mais robusto para .xls)\n",
    "            self.workbook = xlrd.open_workbook(str(self.arquivo_path))\n",
    "            self._log(f\"✅ Carregado com xlrd\")\n",
    "            self._log(f\"   Formato: XLS (Excel Antigo)\")\n",
    "        except Exception as e1:\n",
    "            # Fallback para pandas (xlsx, xlsm)\n",
    "            try:\n",
    "                self.workbook = pd.ExcelFile(str(self.arquivo_path))\n",
    "                self._log(f\"✅ Carregado com pandas\")\n",
    "                self._log(f\"   Formato: XLSX/XLSM (Excel Moderno)\")\n",
    "            except Exception as e2:\n",
    "                raise ValueError(f\"❌ Não foi possível carregar arquivo:\\n  xlrd: {e1}\\n  pandas: {e2}\")\n",
    "\n",
    "        # Listar sheets\n",
    "        if isinstance(self.workbook, xlrd.Book):\n",
    "            sheets = self.workbook.sheet_names()\n",
    "        else:\n",
    "            sheets = self.workbook.sheet_names\n",
    "\n",
    "        self._log(f\"📊 Sheets encontradas: {len(sheets)}\")\n",
    "        for i, sheet in enumerate(sheets, 1):\n",
    "            self._log(f\"   {i}. {sheet}\")\n",
    "\n",
    "    def _detectar_sheet(self):\n",
    "        \"\"\"\n",
    "        Detecta sheet com dados relevantes\n",
    "        \"\"\"\n",
    "        self._log(\"\\n📊 FASE 2: Detecção de Sheet\", nivel=\"SECAO\")\n",
    "\n",
    "        if isinstance(self.workbook, xlrd.Book):\n",
    "            sheets = self.workbook.sheet_names()\n",
    "        else:\n",
    "            sheets = self.workbook.sheet_names\n",
    "\n",
    "        # REGRAS DE DETECÇÃO (ordem de prioridade)\n",
    "        regras = [\n",
    "            # BW específico\n",
    "            (r\"Valor da Variação Total\", \"BW - Variação Total\"),\n",
    "            (r\"Variação.*Total\", \"Variação Total (genérico)\"),\n",
    "            (r\"OPAV\", \"OPAV\"),\n",
    "            # Genéricas\n",
    "            (r\"(?i)dados\", \"Dados\"),\n",
    "            (r\"(?i)relat[oó]rio\", \"Relatório\"),\n",
    "            (r\"(?i)export\", \"Export\"),\n",
    "            (r\"(?i)result\", \"Result\"),\n",
    "        ]\n",
    "\n",
    "        candidatos = []\n",
    "\n",
    "        for sheet_name in sheets:\n",
    "            for padrao, descricao in regras:\n",
    "                if re.search(padrao, sheet_name):\n",
    "                    # Carregar amostra para validar\n",
    "                    validacao = self._validar_sheet(sheet_name)\n",
    "                    candidatos.append({\n",
    "                        'nome': sheet_name,\n",
    "                        'descricao': descricao,\n",
    "                        'score': validacao['score'],\n",
    "                        'linhas': validacao['linhas'],\n",
    "                        'colunas': validacao['colunas']\n",
    "                    })\n",
    "                    self._log(f\"   ✅ Candidato: '{sheet_name}' ({descricao})\")\n",
    "                    self._log(f\"      Score: {validacao['score']:.2f} | Linhas: {validacao['linhas']} | Colunas: {validacao['colunas']}\")\n",
    "                    break\n",
    "\n",
    "        if not candidatos:\n",
    "            # Se nenhum match, usar primeira sheet não vazia\n",
    "            self._log(\"   ⚠️  Nenhum match por padrão, analisando todas sheets...\")\n",
    "            for sheet_name in sheets:\n",
    "                validacao = self._validar_sheet(sheet_name)\n",
    "                if validacao['score'] > 0:\n",
    "                    candidatos.append({\n",
    "                        'nome': sheet_name,\n",
    "                        'descricao': 'Primeira não vazia',\n",
    "                        'score': validacao['score'],\n",
    "                        'linhas': validacao['linhas'],\n",
    "                        'colunas': validacao['colunas']\n",
    "                    })\n",
    "\n",
    "        if not candidatos:\n",
    "            raise ValueError(\"❌ Nenhuma sheet com dados foi encontrada\")\n",
    "\n",
    "        # Selecionar melhor candidato (maior score)\n",
    "        melhor = max(candidatos, key=lambda x: x['score'])\n",
    "        self.sheet_detectada = melhor['nome']\n",
    "\n",
    "        self._log(f\"\\n   🎯 SHEET SELECIONADA: '{self.sheet_detectada}'\")\n",
    "        self._log(f\"      {melhor['descricao']} | Score: {melhor['score']:.2f}\")\n",
    "\n",
    "    def _validar_sheet(self, sheet_name):\n",
    "        \"\"\"\n",
    "        Valida se sheet contém dados úteis\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if isinstance(self.workbook, xlrd.Book):\n",
    "                sheet = self.workbook.sheet_by_name(sheet_name)\n",
    "                linhas = sheet.nrows\n",
    "                colunas = sheet.ncols\n",
    "                # Amostra: primeira linha não vazia\n",
    "                amostra = []\n",
    "                for i in range(min(50, linhas)):\n",
    "                    row = sheet.row_values(i)\n",
    "                    if any(str(c).strip() for c in row):\n",
    "                        amostra.append(row)\n",
    "                        if len(amostra) >= 10:\n",
    "                            break\n",
    "            else:\n",
    "                df_sample = pd.read_excel(self.workbook, sheet_name=sheet_name, nrows=50)\n",
    "                linhas = len(df_sample)\n",
    "                colunas = len(df_sample.columns)\n",
    "                amostra = df_sample.values.tolist()\n",
    "\n",
    "            # Calcular score\n",
    "            score = 0.0\n",
    "            if linhas > 10:\n",
    "                score += 1.0\n",
    "            if colunas > 5:\n",
    "                score += 1.0\n",
    "            if linhas > 100:\n",
    "                score += 0.5\n",
    "            if colunas > 15:\n",
    "                score += 0.5\n",
    "\n",
    "            # Penalizar sheets muito pequenas\n",
    "            if linhas < 5 or colunas < 3:\n",
    "                score = 0.0\n",
    "\n",
    "            return {\n",
    "                'score': score,\n",
    "                'linhas': linhas,\n",
    "                'colunas': colunas\n",
    "            }\n",
    "        except:\n",
    "            return {'score': 0.0, 'linhas': 0, 'colunas': 0}\n",
    "\n",
    "    def _detectar_cabecalho(self):\n",
    "        \"\"\"\n",
    "        Detecta linha de cabeçalho automaticamente\n",
    "        \"\"\"\n",
    "        self._log(\"\\n🔍 FASE 3: Detecção de Cabeçalho\", nivel=\"SECAO\")\n",
    "\n",
    "        # Carregar primeiras 100 linhas\n",
    "        if isinstance(self.workbook, xlrd.Book):\n",
    "            sheet = self.workbook.sheet_by_name(self.sheet_detectada)\n",
    "            linhas_amostra = []\n",
    "            for i in range(min(100, sheet.nrows)):\n",
    "                linhas_amostra.append(sheet.row_values(i))\n",
    "        else:\n",
    "            df_amostra = pd.read_excel(\n",
    "                self.workbook,\n",
    "                sheet_name=self.sheet_detectada,\n",
    "                nrows=100,\n",
    "                header=None\n",
    "            )\n",
    "            linhas_amostra = df_amostra.values.tolist()\n",
    "\n",
    "        # Análise linha por linha\n",
    "        scores = []\n",
    "        for idx, linha in enumerate(linhas_amostra):\n",
    "            score = self._avaliar_linha_cabecalho(linha, idx)\n",
    "            scores.append({\n",
    "                'linha': idx,\n",
    "                'score': score,\n",
    "                'conteudo_sample': linha[:5]  # Primeiras 5 colunas\n",
    "            })\n",
    "\n",
    "        # Ordenar por score\n",
    "        scores_ordenados = sorted(scores, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "        # Mostrar top 5\n",
    "        self._log(\"   🏆 Top 5 candidatos a cabeçalho:\")\n",
    "        for i, item in enumerate(scores_ordenados[:5], 1):\n",
    "            self._log(f\"      {i}. Linha {item['linha']+1} (Excel) - Score: {item['score']:.2f}\")\n",
    "            self._log(f\"         Sample: {item['conteudo_sample']}\")\n",
    "\n",
    "        # Selecionar melhor\n",
    "        melhor = scores_ordenados[0]\n",
    "        self.linha_cabecalho = melhor['linha']\n",
    "        self.linha_dados_inicio = self.linha_cabecalho + 1\n",
    "\n",
    "        self._log(f\"\\n   🎯 CABEÇALHO DETECTADO: Linha {self.linha_cabecalho + 1} (Excel)\")\n",
    "        self._log(f\"      Início dos dados: Linha {self.linha_dados_inicio + 1} (Excel)\")\n",
    "\n",
    "    def _avaliar_linha_cabecalho(self, linha, idx):\n",
    "        \"\"\"\n",
    "        Avalia se uma linha é candidata a cabeçalho\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "\n",
    "        # Converter para strings\n",
    "        celulas = [str(c).strip() for c in linha if str(c).strip()]\n",
    "\n",
    "        if not celulas:\n",
    "            return 0.0\n",
    "\n",
    "        # CRITÉRIO 1: Quantidade de células não vazias (peso 2.0)\n",
    "        prop_nao_vazias = len(celulas) / len(linha)\n",
    "        score += prop_nao_vazias * 2.0\n",
    "\n",
    "        # CRITÉRIO 2: Células com texto (não só números) (peso 1.5)\n",
    "        tem_texto = sum(1 for c in celulas if re.search(r'[a-zA-Z]', c))\n",
    "        prop_texto = tem_texto / len(celulas) if celulas else 0\n",
    "        score += prop_texto * 1.5\n",
    "\n",
    "        # CRITÉRIO 3: Palavras-chave de cabeçalho (peso 3.0)\n",
    "        keywords = [\n",
    "            'centro', 'produto', 'material', 'código', 'cod', 'cód',\n",
    "            'data', 'período', 'ano', 'mês', 'mes',\n",
    "            'quantidade', 'valor', 'volume', 'expedição', 'variação',\n",
    "            'limite', 'batente', 'sigla', 'base', 'região', 'regiao',\n",
    "            'nome', 'descrição', 'descricao', 'tipo', 'status'\n",
    "        ]\n",
    "\n",
    "        texto_linha = ' '.join(celulas).lower()\n",
    "        keywords_encontradas = sum(1 for kw in keywords if kw in texto_linha)\n",
    "        score += (keywords_encontradas / len(keywords)) * 3.0\n",
    "\n",
    "        # CRITÉRIO 4: Tamanho médio das células (cabeçalhos tendem a ser curtos) (peso 1.0)\n",
    "        tamanho_medio = np.mean([len(c) for c in celulas])\n",
    "        if 5 <= tamanho_medio <= 50:\n",
    "            score += 1.0\n",
    "        elif tamanho_medio > 100:\n",
    "            score -= 0.5  # Penalizar linhas muito longas\n",
    "\n",
    "        # CRITÉRIO 5: Unicidade (cabeçalhos não devem ter repetições) (peso 1.5)\n",
    "        contador = Counter(celulas)\n",
    "        repeticoes = sum(1 for c, n in contador.items() if n > 1)\n",
    "        if repeticoes == 0:\n",
    "            score += 1.5\n",
    "        else:\n",
    "            score -= repeticoes * 0.3\n",
    "\n",
    "        # CRITÉRIO 6: Posição (linhas mais acima têm vantagem) (peso 0.5)\n",
    "        if idx < 50:\n",
    "            score += (50 - idx) / 100\n",
    "\n",
    "        # CRITÉRIO 7: Formato típico BW (linha ~30-40)\n",
    "        if 30 <= idx <= 40:\n",
    "            score += 0.5\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _extrair_dados(self):\n",
    "        \"\"\"\n",
    "        Extrai dados a partir da linha detectada\n",
    "        \"\"\"\n",
    "        self._log(\"\\n📊 FASE 4: Extração de Dados\", nivel=\"SECAO\")\n",
    "\n",
    "        if isinstance(self.workbook, xlrd.Book):\n",
    "            sheet = self.workbook.sheet_by_name(self.sheet_detectada)\n",
    "\n",
    "            # Extrair todas as linhas\n",
    "            data = []\n",
    "            for i in range(sheet.nrows):\n",
    "                data.append(sheet.row_values(i))\n",
    "\n",
    "            self.df_bruto = pd.DataFrame(data)\n",
    "\n",
    "            # Definir cabeçalho\n",
    "            cabecalho_bruto = self.df_bruto.iloc[self.linha_cabecalho].tolist()\n",
    "\n",
    "            # Extrair dados\n",
    "            self.df_bruto = self.df_bruto.iloc[self.linha_dados_inicio:].copy()\n",
    "            self.df_bruto.columns = cabecalho_bruto\n",
    "\n",
    "        else:\n",
    "            # pandas\n",
    "            self.df_bruto = pd.read_excel(\n",
    "                self.workbook,\n",
    "                sheet_name=self.sheet_detectada,\n",
    "                header=self.linha_cabecalho\n",
    "            )\n",
    "\n",
    "        self.df_bruto = self.df_bruto.reset_index(drop=True)\n",
    "\n",
    "        self._log(f\"✅ Dados extraídos\")\n",
    "        self._log(f\"   Registros: {len(self.df_bruto):,}\")\n",
    "        self._log(f\"   Colunas: {len(self.df_bruto.columns)}\")\n",
    "\n",
    "    def _limpar_estrutura(self):\n",
    "        \"\"\"\n",
    "        Limpa estrutura do DataFrame\n",
    "        \"\"\"\n",
    "        self._log(\"\\n🧹 FASE 5: Limpeza de Estrutura\", nivel=\"SECAO\")\n",
    "\n",
    "        df = self.df_bruto.copy()\n",
    "\n",
    "        # 1. Remover colunas completamente vazias\n",
    "        colunas_vazias = df.columns[df.isna().all()].tolist()\n",
    "        if colunas_vazias:\n",
    "            self._log(f\"   🗑️  Removendo {len(colunas_vazias)} colunas vazias\")\n",
    "            df = df.drop(columns=colunas_vazias)\n",
    "\n",
    "        # 2. Remover linhas completamente vazias\n",
    "        linhas_vazias = df.index[df.isna().all(axis=1)].tolist()\n",
    "        if linhas_vazias:\n",
    "            self._log(f\"   🗑️  Removendo {len(linhas_vazias)} linhas vazias\")\n",
    "            df = df.dropna(how='all')\n",
    "\n",
    "        # 3. Limpar nomes de colunas\n",
    "        self._log(\"   🧹 Limpando nomes de colunas...\")\n",
    "        colunas_limpas = []\n",
    "        for col in df.columns:\n",
    "            col_limpo = str(col).strip()\n",
    "            col_limpo = col_limpo.lstrip(\"'\")  # Excel adiciona '\n",
    "            col_limpo = col_limpo.replace('\\n', ' ')\n",
    "            col_limpo = col_limpo.replace('\\r', '')\n",
    "            col_limpo = ' '.join(col_limpo.split())  # Múltiplos espaços\n",
    "            colunas_limpas.append(col_limpo)\n",
    "\n",
    "        df.columns = colunas_limpas\n",
    "\n",
    "        # 4. Renomear colunas duplicadas\n",
    "        contagem = Counter(colunas_limpas)\n",
    "        duplicadas = {c: n for c, n in contagem.items() if n > 1}\n",
    "\n",
    "        if duplicadas:\n",
    "            self._log(f\"   ⚠️  Renomeando {len(duplicadas)} colunas duplicadas:\")\n",
    "            colunas_finais = []\n",
    "            contador = {}\n",
    "\n",
    "            for col in colunas_limpas:\n",
    "                if col in duplicadas:\n",
    "                    if col not in contador:\n",
    "                        contador[col] = 0\n",
    "                        colunas_finais.append(col)\n",
    "                    else:\n",
    "                        contador[col] += 1\n",
    "                        novo_nome = f\"{col}_dup{contador[col]}\"\n",
    "                        colunas_finais.append(novo_nome)\n",
    "                        self._log(f\"      '{col}' → '{novo_nome}'\")\n",
    "                else:\n",
    "                    colunas_finais.append(col)\n",
    "\n",
    "            df.columns = colunas_finais\n",
    "\n",
    "        # 5. Remover linhas de totais/resultados\n",
    "        padroes_remover = [\n",
    "            r'(?i)^total',\n",
    "            r'(?i)^resultado',\n",
    "            r'(?i)^soma',\n",
    "            r'(?i)^subtotal',\n",
    "            r'(?i)^grand total'\n",
    "        ]\n",
    "\n",
    "        linhas_remover = []\n",
    "        for idx, row in df.iterrows():\n",
    "            primeira_celula = str(row.iloc[0]).strip().lower()\n",
    "            for padrao in padroes_remover:\n",
    "                if re.search(padrao, primeira_celula):\n",
    "                    linhas_remover.append(idx)\n",
    "                    break\n",
    "\n",
    "        if linhas_remover:\n",
    "            self._log(f\"   🗑️  Removendo {len(linhas_remover)} linhas de totais/resultados\")\n",
    "            df = df.drop(index=linhas_remover)\n",
    "\n",
    "        # 6. Reset index\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "        self.df_limpo = df\n",
    "\n",
    "        self._log(f\"\\n✅ Limpeza concluída\")\n",
    "        self._log(f\"   Registros finais: {len(self.df_limpo):,}\")\n",
    "        self._log(f\"   Colunas finais: {len(self.df_limpo.columns)}\")\n",
    "\n",
    "    def _gerar_relatorio(self):\n",
    "        \"\"\"\n",
    "        Gera relatório final de detecção\n",
    "        \"\"\"\n",
    "        self._log(\"\\n\" + \"=\"*80, nivel=\"TITULO\")\n",
    "        self._log(\"📋 RELATÓRIO FINAL DE DETECÇÃO\", nivel=\"TITULO\")\n",
    "        self._log(\"=\"*80, nivel=\"TITULO\")\n",
    "\n",
    "        self._log(f\"\\n📁 Arquivo: {self.nome_arquivo}\")\n",
    "        self._log(f\"📊 Sheet: {self.sheet_detectada}\")\n",
    "        self._log(f\"📍 Cabeçalho: Linha {self.linha_cabecalho + 1} (Excel)\")\n",
    "        self._log(f\"📍 Dados: Linha {self.linha_dados_inicio + 1} (Excel)\")\n",
    "\n",
    "        self._log(f\"\\n📊 Resultado Final:\")\n",
    "        self._log(f\"   Registros: {len(self.df_limpo):,}\")\n",
    "        self._log(f\"   Colunas: {len(self.df_limpo.columns)}\")\n",
    "\n",
    "        self._log(f\"\\n📋 Colunas detectadas:\")\n",
    "        for i, col in enumerate(self.df_limpo.columns, 1):\n",
    "            self._log(f\"   {i:2d}. {col}\")\n",
    "\n",
    "    def _log(self, mensagem, nivel=\"INFO\"):\n",
    "        \"\"\"\n",
    "        Sistema de log\n",
    "        \"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "        if nivel == \"TITULO\":\n",
    "            print(mensagem)\n",
    "        elif nivel == \"SECAO\":\n",
    "            print(f\"\\n{mensagem}\")\n",
    "            print(\"─\" * 80)\n",
    "        else:\n",
    "            print(mensagem)\n",
    "\n",
    "        self.log.append({\n",
    "            'timestamp': timestamp,\n",
    "            'nivel': nivel,\n",
    "            'mensagem': mensagem\n",
    "        })\n",
    "\n",
    "    def exportar_log(self, pasta_destino):\n",
    "        \"\"\"\n",
    "        Exporta log para arquivo\n",
    "        \"\"\"\n",
    "        log_df = pd.DataFrame(self.log)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        arquivo_log = Path(pasta_destino) / f\"LOG_DeteccaoAutomatica_{timestamp}.xlsx\"\n",
    "\n",
    "        log_df.to_excel(arquivo_log, index=False)\n",
    "        print(f\"\\n💾 Log salvo: {arquivo_log}\")\n",
    "\n",
    "        return arquivo_log\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "# EXEMPLO DE USO\n",
    "# ═══════════════════════════════════════════════════════════════════\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Usar FileManager se disponível\n",
    "    if 'fm' in dir():\n",
    "        pasta_entrada = fm.diretorios['dados_entrada']\n",
    "        pasta_logs = fm.diretorios['logs']\n",
    "    else:\n",
    "        pasta_entrada = Path('02_Dados_Entrada')\n",
    "        pasta_logs = Path('logs')\n",
    "\n",
    "    # Selecionar arquivo\n",
    "    arquivos_disponiveis = list(pasta_entrada.glob('*.xls*'))\n",
    "\n",
    "    print(\"📁 Arquivos disponíveis:\")\n",
    "    for i, arq in enumerate(arquivos_disponiveis, 1):\n",
    "        print(f\"   {i}. {arq.name}\")\n",
    "\n",
    "    escolha = int(input(\"\\nEscolha um arquivo (número): \")) - 1\n",
    "    arquivo_selecionado = arquivos_disponiveis[escolha]\n",
    "\n",
    "    # Processar\n",
    "    detector = DetectorArquivoDesconhecido(arquivo_selecionado)\n",
    "    df_resultado = detector.processar()\n",
    "\n",
    "    # Exportar log\n",
    "    detector.exportar_log(pasta_logs)\n",
    "\n",
    "    # Salvar resultado\n",
    "    arquivo_saida = pasta_entrada.parent / '03_Dados_Processados' / f\"Detectado_{arquivo_selecionado.stem}.xlsx\"\n",
    "    df_resultado.to_excel(arquivo_saida, index=False)\n",
    "    print(f\"💾 Resultado salvo: {arquivo_saida}\")"
   ],
   "id": "4a5f4951d942e0c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "34b56a7611fbc6aa",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
